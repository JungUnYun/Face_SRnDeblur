{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRnDeblur_joint 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/SRnDeblurN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TTF\n",
    "import torchvision.models as models\n",
    "from torch.backends import cudnn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/generator.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        bn = None\n",
    "        if batch_size == 1:\n",
    "            bn = False # Instance Normalization\n",
    "        else:\n",
    "            bn = True # Batch Normalization\n",
    "\n",
    "        #============================ upscale ============================#\n",
    "        self.upscale8 = nn.Sequential(\n",
    "            # [3x32x32] -> [64x32x32]\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, stride=1, padding=4, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # [64x32x32] -> [256x32x32]\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # [256x32x32] -> [64x64x64]\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # [64x64x64] -> [256x64x64]\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # [256x64x64] -> [64x128x128]\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # [64x128x128] -> [256x128x128]\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # [256x128x128] -> [64x256x256]\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  \n",
    "            # [64x256x256] -> [3x256x256]\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        )\n",
    "        #============================ upscale ============================#\n",
    "\n",
    "\n",
    "        # nn.Conv2d(input channel 수, convolution에 의해 생성된 channel 수, kernel size, stride=default 1, padding=default 0)\n",
    "        # [3x256x256] -> [64x128x128]\n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        # [64x256x256] -> [64x128x128]\n",
    "#         self.conv1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "\n",
    "        # -> [128x64x64]\n",
    "        conv2 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv2 += [nn.BatchNorm2d(128)]\n",
    "        else:\n",
    "            conv2 += [nn.InstanceNorm2d(128)]\n",
    "        self.conv2 = nn.Sequential(*conv2)\n",
    "\n",
    "        # -> [256x32x32]\n",
    "        conv3 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(128, 256, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv3 += [nn.BatchNorm2d(256)]\n",
    "        else:\n",
    "            conv3 += [nn.InstanceNorm2d(256)]\n",
    "        self.conv3 = nn.Sequential(*conv3)\n",
    "\n",
    "        # -> [512x16x16]\n",
    "        conv4 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(256, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv4 += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            conv4 += [nn.InstanceNorm2d(512)]\n",
    "        self.conv4 = nn.Sequential(*conv4)\n",
    "\n",
    "        # -> [512x8x8]\n",
    "        conv5 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(512, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv5 += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            conv5 += [nn.InstanceNorm2d(512)]\n",
    "        self.conv5 = nn.Sequential(*conv5)\n",
    "\n",
    "        # -> [512x4x4]\n",
    "        conv6 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(512, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv6 += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            conv6 += [nn.InstanceNorm2d(512)]\n",
    "        self.conv6 = nn.Sequential(*conv6)\n",
    "\n",
    "        # -> [512x2x2]\n",
    "        conv7 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(512, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv7 += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            conv7 += [nn.InstanceNorm2d(512)]\n",
    "        self.conv7 = nn.Sequential(*conv7)\n",
    "\n",
    "        # -> [512x1x1]\n",
    "        conv8 = [nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Conv2d(512, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            conv8 += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            conv8 += [nn.InstanceNorm2d(512)]\n",
    "        self.conv8 = nn.Sequential(*conv8)\n",
    "\n",
    "        # -> [512x2x2]\n",
    "        deconv8 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(512, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv8 += [nn.BatchNorm2d(512), nn.Dropout(0.5)]\n",
    "        else:\n",
    "            deconv8 += [nn.InstanceNorm2d(512), nn.Dropout(0.5)]\n",
    "        self.deconv8 = nn.Sequential(*deconv8)\n",
    "\n",
    "        # [(512+512)x2x2] -> [512x4x4]\n",
    "        deconv7 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(512 * 2, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv7 += [nn.BatchNorm2d(512), nn.Dropout(0.5)]\n",
    "        else:\n",
    "            deconv7 += [nn.InstanceNorm2d(512), nn.Dropout(0.5)]\n",
    "        self.deconv7 = nn.Sequential(*deconv7)\n",
    "\n",
    "        # [(512+512)x4x4] -> [512x8x8]\n",
    "        deconv6 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(512 * 2, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv6 += [nn.BatchNorm2d(512), nn.Dropout(0.5)]\n",
    "        else:\n",
    "            deconv6 += [nn.InstanceNorm2d(512), nn.Dropout(0.5)]\n",
    "        self.deconv6 = nn.Sequential(*deconv6)\n",
    "\n",
    "        # [(512+512)x8x8] -> [512x16x16]\n",
    "        deconv5 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(512 * 2, 512, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv5 += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            deconv5 += [nn.InstanceNorm2d(512)]\n",
    "        self.deconv5 = nn.Sequential(*deconv5)\n",
    "\n",
    "        # [(512+512)x16x16] -> [256x32x32]\n",
    "        deconv4 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(512 * 2, 256, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv4 += [nn.BatchNorm2d(256)]\n",
    "        else:\n",
    "            deconv4 += [nn.InstanceNorm2d(256)]\n",
    "        self.deconv4 = nn.Sequential(*deconv4)\n",
    "        \n",
    "        # [(512+512)x16x16] -> [256x32x32]\n",
    "        deconv4_0 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(512 * 1, 256, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv4_0 += [nn.BatchNorm2d(256)]\n",
    "        else:\n",
    "            deconv4_0 += [nn.InstanceNorm2d(256)]\n",
    "        self.deconv4_0 = nn.Sequential(*deconv4_0)        \n",
    "\n",
    "        # [(256+256)x32x32] -> [128x64x64]\n",
    "        deconv3 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(256 * 2, 128, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv3 += [nn.BatchNorm2d(128)]\n",
    "        else:\n",
    "            deconv3 += [nn.InstanceNorm2d(128)]\n",
    "        self.deconv3 = nn.Sequential(*deconv3)\n",
    "\n",
    "        # [(256+256)x32x32] -> [128x64x64]\n",
    "        deconv3_0 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(256 * 2, 128, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv3_0 += [nn.BatchNorm2d(128)]\n",
    "        else:\n",
    "            deconv3_0 += [nn.InstanceNorm2d(128)]\n",
    "        self.deconv3_0 = nn.Sequential(*deconv3_0)\n",
    "        \n",
    "        # [(128+128)x64x64] -> [64x128x128]\n",
    "        deconv2 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(128 * 2, 64, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv2 += [nn.BatchNorm2d(64)]\n",
    "        else:\n",
    "            deconv2 += [nn.InstanceNorm2d(64)]\n",
    "        self.deconv2 = nn.Sequential(*deconv2)\n",
    "        \n",
    "        # [(128+128)x64x64] -> [64x128x128]\n",
    "        deconv2_0 = [nn.ReLU(),\n",
    "                   nn.ConvTranspose2d(128 * 2, 64, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            deconv2_0 += [nn.BatchNorm2d(64)]\n",
    "        else:\n",
    "            deconv2_0 += [nn.InstanceNorm2d(64)]\n",
    "        self.deconv2_0 = nn.Sequential(*deconv2_0)\n",
    "\n",
    "        # [(64+64)x128x128] -> [3x256x256]\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64 * 2, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # [(64+64)x128x128] -> [3x256x256]\n",
    "        self.deconv1_0 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64 * 2, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # NCHW = H*W\n",
    "        \n",
    "        noise = torch.empty_like(x).normal_(mean=1.0,std=0.1)\n",
    "        inN = x + noise\n",
    "        upx = self.upscale8(inN)\n",
    "        \n",
    "        c1 = self.conv1(upx)\n",
    "        c1_1 = torch.empty_like(c1).normal_(mean=1.0,std=0.1)\n",
    "        c1_n = c1 + c1_1\n",
    "        c2 = self.conv2(c1_n)\n",
    "        c2_1 = torch.empty_like(c2).normal_(mean=1.0,std=0.1)\n",
    "        c2_n = c2 + c2_1\n",
    "        c3 = self.conv3(c2_n)\n",
    "        c3_1 = torch.empty_like(c3).normal_(mean=1.0,std=0.1)\n",
    "        c3_n = c3 + c3_1\n",
    "        c4 = self.conv4(c3_n)\n",
    "        c4_1 = torch.empty_like(c4).normal_(mean=1.0,std=0.1)\n",
    "        c4_n = c4 + c4_1\n",
    "        c5 = self.conv5(c4_n)\n",
    "        c5_1 = torch.empty_like(c5).normal_(mean=1.0,std=0.1)\n",
    "        c5_n = c5 + c5_1\n",
    "        c6 = self.conv6(c5_n)\n",
    "        c6_1 = torch.empty_like(c6).normal_(mean=1.0,std=0.1)\n",
    "        c6_n = c6 + c6_1\n",
    "        c7 = self.conv7(c6_n)\n",
    "        c7_1 = torch.empty_like(c7).normal_(mean=1.0,std=0.1)\n",
    "        c7_n = c7 + c7_1\n",
    "        c8 = self.conv8(c7_n)\n",
    "        c8_1 = torch.empty_like(c8).normal_(mean=1.0,std=0.1)\n",
    "        c8_n = c8 + c8_1\n",
    "        \n",
    "        d3_0 = self.deconv4_0(c4_n)\n",
    "        d3_0 = torch.cat((c3,d3_0), dim=1)\n",
    "        d3_1 = torch.empty_like(d3_0).normal_(mean=1.0,std=0.1)\n",
    "        d3_n = d3_0 + d3_1\n",
    "        d2_0 = self.deconv3_0(d3_n)\n",
    "        d2_0 = torch.cat((c2,d2_0), dim=1)\n",
    "        d2_1 = torch.empty_like(d2_0).normal_(mean=1.0,std=0.1)\n",
    "        d2_n = d2_0 + d2_1\n",
    "        d1_0 = self.deconv2_0(d2_n)\n",
    "        d1_00 = torch.cat((c1,d1_0), dim=1)    \n",
    "        d1_1 = torch.empty_like(d1_00).normal_(mean=1.0,std=0.1)\n",
    "        d1_n = d1_00 + d1_1\n",
    "        outLR = self.deconv1_0(d1_n)\n",
    "        \n",
    "        d7 = self.deconv8(c8_n)\n",
    "        d7 = torch.cat((c7, d7), dim=1)\n",
    "        d17_1 = torch.empty_like(d7).normal_(mean=1.0,std=0.1)\n",
    "        d17_n = d7 + d17_1\n",
    "        d6 = self.deconv7(d17_n)\n",
    "        d6 = torch.cat((c6, d6), dim=1)\n",
    "        d16_1 = torch.empty_like(d6).normal_(mean=1.0,std=0.1)\n",
    "        d16_n = d6 + d16_1\n",
    "        d5 = self.deconv6(d16_n)\n",
    "        d5 = torch.cat((c5, d5), dim=1)\n",
    "        d15_1 = torch.empty_like(d5).normal_(mean=1.0,std=0.1)\n",
    "        d15_n = d5 + d15_1\n",
    "        d4 = self.deconv5(d15_n)\n",
    "        d4 = torch.cat((c4, d4), dim=1)\n",
    "        d14_1 = torch.empty_like(d4).normal_(mean=1.0,std=0.1)\n",
    "        d14_n = d4 + d14_1\n",
    "        d3 = self.deconv4(d14_n)\n",
    "        d3 = torch.cat((c3, d3), dim=1)\n",
    "        d13_1 = torch.empty_like(d3).normal_(mean=1.0,std=0.1)\n",
    "        d13_n = d3 + d13_1\n",
    "        d2 = self.deconv3(d13_n)\n",
    "        d2 = torch.cat((c2, d2), dim=1)\n",
    "        d12_1 = torch.empty_like(d2).normal_(mean=1.0,std=0.1)\n",
    "        d12_n = d2 + d12_1\n",
    "        d1 = self.deconv2(d12_n)\n",
    "        d1 = torch.add(d1,d1_0)\n",
    "        d1 = torch.cat((c1, d1), dim=1)\n",
    "        d11_1 = torch.empty_like(d1).normal_(mean=1.0,std=0.1)\n",
    "        d11_n = d1 + d11_1\n",
    "        outHR = self.deconv1(d11_n)\n",
    "#         output = torch.add(outLR,outHR)\n",
    "#         d1 = torch.cat((c1, d1), dim=1)\n",
    "#         outHR = self.deconv1(d1)\n",
    "\n",
    "\n",
    "#         return outLR, outHR\n",
    "        return upx,outLR, outHR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/discriminator.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, inch, batch_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        bn = None\n",
    "        if batch_size == 1:\n",
    "            bn = False  # Instance Normalization\n",
    "        else:\n",
    "            bn = True  # Batch Normalization\n",
    "\n",
    "#         # [(3+3)x256x256] -> [64x128x128] -> [128x64x64]\n",
    "#         main = [nn.Conv2d(3*2, 64, 4, 2, 1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(64, 128, 4, 2, 1)]\n",
    "        # [(4+4)x256x256] -> [64x128x128] -> [128x64x64]\n",
    "        main = [nn.Conv2d(inch, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1)]        \n",
    "        if bn == True:\n",
    "            main += [nn.BatchNorm2d(128)]\n",
    "        else:\n",
    "            main += [nn.InstanceNorm2d(128)]\n",
    "\n",
    "        # -> [256x32x32]\n",
    "        main += [nn.LeakyReLU(0.2, inplace=True),\n",
    "                  nn.Conv2d(128, 256, 4, 2, 1)]\n",
    "        if bn == True:\n",
    "            main += [nn.BatchNorm2d(256)]\n",
    "        else:\n",
    "            main += [nn.InstanceNorm2d(256)]\n",
    "\n",
    "        # -> [512x31x31] (Fully Convolutional)\n",
    "        main += [nn.LeakyReLU(0.2, inplace=True),\n",
    "                  nn.Conv2d(256, 512, 4, 1, 1)]\n",
    "        if bn == True:\n",
    "            main += [nn.BatchNorm2d(512)]\n",
    "        else:\n",
    "            main += [nn.InstanceNorm2d(512)]\n",
    "\n",
    "        # -> [1x30x30] (Fully Convolutional, PatchGAN)\n",
    "        main += [nn.LeakyReLU(0.2, inplace=True),\n",
    "                  nn.Conv2d(512, 1, 4, 1, 1)]\n",
    "                  #nn.Sigmoid()]\n",
    "\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "#     def forward(self, x1, x2): # One for Real, One for Fake\n",
    "#         out = torch.cat((x1, x2), dim=1)\n",
    "#         return self.main(out)   \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Vgg19(torch.nn.Module):\n",
    "#     def __init__(self, requires_grad=False):\n",
    "#         super(Vgg19, self).__init__()\n",
    "#         vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
    "#         self.slice1 = torch.nn.Sequential()\n",
    "#         self.slice2 = torch.nn.Sequential()\n",
    "#         self.slice3 = torch.nn.Sequential()\n",
    "#         self.slice4 = torch.nn.Sequential()\n",
    "#         self.slice5 = torch.nn.Sequential()\n",
    "#         for x in range(2):\n",
    "#             self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "#         for x in range(2, 7):\n",
    "#             self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "#         for x in range(7, 12):\n",
    "#             self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "#         for x in range(12, 21):\n",
    "#             self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "#         for x in range(21, 30):\n",
    "#             self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "#         if not requires_grad:\n",
    "#             for param in self.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         h_relu1 = self.slice1(X)\n",
    "#         h_relu2 = self.slice2(h_relu1)\n",
    "#         h_relu3 = self.slice3(h_relu2)\n",
    "#         h_relu4 = self.slice4(h_relu3)\n",
    "#         h_relu5 = self.slice5(h_relu4)\n",
    "#         out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "#         return out\n",
    "    \n",
    "# class VGGLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGGLoss, self).__init__()\n",
    "#         self.vgg = Vgg19().cuda()\n",
    "#         self.criterion = nn.L1Loss()\n",
    "#         self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "#         loss = 0\n",
    "#         for i in range(len(x_vgg)):\n",
    "#             loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
    "#         return loss\n",
    "\n",
    "# VGG16-face\n",
    "class VGG_16(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.block_size = [2, 2, 3, 3, 3]\n",
    "        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.fc6 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 2622)\n",
    "\n",
    "    def load_weights(self, path=\"pretrained/VGG_FACE.t7\"):\n",
    "\n",
    "        model = torchfile.load(path)\n",
    "        counter = 1\n",
    "        block = 1\n",
    "        for i, layer in enumerate(model.modules):\n",
    "            if layer.weight is not None:\n",
    "                if block <= 5:\n",
    "                    self_layer = getattr(self, \"conv_%d_%d\" % (block, counter))\n",
    "                    counter += 1\n",
    "                    if counter > self.block_size[block - 1]:\n",
    "                        counter = 1\n",
    "                        block += 1\n",
    "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
    "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
    "                else:\n",
    "                    self_layer = getattr(self, \"fc%d\" % (block))\n",
    "                    block += 1\n",
    "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
    "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Pytorch forward\n",
    "        Args:\n",
    "            x: input image (224x224)\n",
    "        Returns: class logits\n",
    "        \"\"\"\n",
    "        x = F.upsample(x,(224,224),mode='bilinear')\n",
    "        x = F.relu(self.conv_1_1(x))\n",
    "        x = F.relu(self.conv_1_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_2_1(x))\n",
    "        x = F.relu(self.conv_2_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_3_1(x))\n",
    "        x = F.relu(self.conv_3_2(x))\n",
    "        x = F.relu(self.conv_3_3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_4_1(x))\n",
    "        x = F.relu(self.conv_4_2(x))\n",
    "        x = F.relu(self.conv_4_3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_5_1(x))\n",
    "        x = F.relu(self.conv_5_2(x))\n",
    "        x = F.relu(self.conv_5_3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        return self.fc8(x)\n",
    "    \n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        self.vgg = VGG_16().double().cuda()\n",
    "        \n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
    "        return loss\n",
    "\n",
    "####################################################################################\n",
    "#     model = VGG_16().double()\n",
    "#     model.load_weights()\n",
    "#     im = cv2.imread(\"images/ak.png\")\n",
    "#     im = torch.Tensor(im).permute(2, 0, 1).view(1, 3, 224, 224).double()\n",
    "#     import numpy as np\n",
    "\n",
    "#     model.eval()\n",
    "#     im -= torch.Tensor(np.array([129.1863, 104.7624, 93.5940])).double().view(1, 3, 1, 1)\n",
    "#     preds = F.softmax(model(im), dim=1)\n",
    "#     values, indices = preds.max(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions for Data Loading & Pre-processingclass ImageFolder(data.Dataset):\n",
    "class ImageFolder(data.Dataset):\n",
    "    def __init__(self, opt):\n",
    "        # os.listdir function gives all lists of directory\n",
    "        self.root = opt.dataroot\n",
    "        self.no_resize_or_crop = opt.no_resize_or_crop\n",
    "        self.no_flip = opt.no_flip\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "        self.transformM = transforms.Compose([transforms.ToTensor()])\n",
    "        #=====================================================================================#\n",
    "        self.dir_A = os.path.join(opt.dataroot,'trainx8')\n",
    "        self.Aimg_paths = list(map(lambda x:os.path.join(self.dir_A,x),os.listdir(self.dir_A)))\n",
    "        #=====================================================================================#\n",
    "#         self.dir_AB = os.path.join(opt.dataroot, 'train')\n",
    "#         self.image_paths = list(map(lambda x: os.path.join(self.dir_AB, x), os.listdir(self.dir_AB)))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #=====================================================================================#\n",
    "        # A : 32x32 (blur + LR)\n",
    "        # B : 256x256 (LR)\n",
    "        # C : 256x256 (GT)\n",
    "        # D : 256x256 (fmask)\n",
    "        A_path = self.Aimg_paths[index]\n",
    "        trn = A_path.find('trainx8')\n",
    "        endn = len(A_path)\n",
    "        B_path = A_path[:trn]+'wblur'+A_path[trn+7:endn-4]+'.png'\n",
    "        C_path = A_path[:trn]+'GT'+A_path[trn+7:endn-4]+'.jpg'\n",
    "        D_path = A_path[:trn]+'fmask'+A_path[trn+7:endn-4]+'.png'\n",
    "#         B_path = A_path[:trn]+'GT'+A_path[trn+5:endn-4]+'_mask.jpg'\n",
    "        \n",
    "        A = Image.open(A_path).convert('RGB')\n",
    "        B = Image.open(B_path).convert('RGB')\n",
    "        C = Image.open(C_path).convert('RGB')\n",
    "        D = Image.open(D_path)\n",
    "        E = A.resize((256,256),Image.BICUBIC)\n",
    "#         A = A.resize((256,256),Image.BICUBIC)\n",
    "#         B = (C.resize((32,32),Image.BICUBIC)).resize((256,256),Image.BICUBIC)\n",
    "#         C = C.resize((256,256),Image.BICUBIC)\n",
    "#         D = D.resize((256,256),Image.BICUBIC)\n",
    "#         D = D.resize((256,256),Image.BICUBIC)\n",
    "#         D = torch.zeros(256,256)\n",
    "#         D = TTF.to_pil_image(D)\n",
    "\n",
    "        A = self.transform(A)\n",
    "        B = self.transform(B)\n",
    "        C = self.transform(C)\n",
    "        D = self.transformM(D)\n",
    "        E = self.transform(E)\n",
    "        A = A[:,:32,:32]\n",
    "#             A = A[:,:256,:256]\n",
    "        B = B[:,:256,:256]\n",
    "        C = C[:,:256,:256]\n",
    "        D = D[:,:256,:256]\n",
    "        E = E[:,:256,:256]\n",
    "        \n",
    "#         if (not(self.no_flip)) and random.random() < 0.5:\n",
    "#             idx = [i for i in range(A.size(2)-1,-1,-1)]\n",
    "#             idx = torch.LongTensor(idx)\n",
    "#             A = A.index_select(2,idx)\n",
    "#             B = B.index_select(2,idx)\n",
    "#             C = C.index_select(2,idx)\n",
    "#             D = D.index_select(2,idx)\n",
    "\n",
    "#         A = (torch.cat((A,D),dim=0))[0:4,:,:]\n",
    "#         B = (torch.cat((B,C),dim=0))[0:4,:,:]\n",
    "#         print(A.shape, B.shape, C.shape, D.shape)\n",
    "#         print('A', A.size())\n",
    "#         print('B', B.size())\n",
    "#         print('C', C.size())\n",
    "#         print('D', D.size())\n",
    "        return {'A':A, 'B':B, 'C':C, 'D':D, 'E':E}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Aimg_paths)\n",
    "\n",
    "##### Helper Function for GPU Training\n",
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "##### Helper Function for Math\n",
    "def denorm(x):\n",
    "    out = (x+1)/2\n",
    "    return out.clamp(0,1)\n",
    "\n",
    "##### Helper Functions for GAN Loss (4D Loss Comparison)\n",
    "def GAN_Loss(input, target, criterion):\n",
    "    if target == True:\n",
    "        tmp_tensor = torch.FloatTensor(input.size()).fill_(1.0)\n",
    "        labels = Variable(tmp_tensor, requires_grad=False)\n",
    "    else:\n",
    "        tmp_tensor = torch.FloatTensor(input.size()).fill_(0.0)\n",
    "        labels = Variable(tmp_tensor, requires_grad=False)\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    return criterion(input, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_workers'], dest='num_workers', nargs=None, const=None, default=2, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU using\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "parser = argparse.ArgumentParser(description='Implementation of SRnDeblur')\n",
    "\n",
    "# Task\n",
    "parser.add_argument('--dataroot', required=True, help='path to images (should have subfolders train, val, etc)')\n",
    "parser.add_argument('--which_direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
    "\n",
    "# Pre-processing\n",
    "parser.add_argument('--no_resize_or_crop', action='store_true', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]')\n",
    "parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
    "\n",
    "# Hyper-parameters\n",
    "parser.add_argument('--num_epochs', type=int, default=100)\n",
    "parser.add_argument('--batchSize', type=int, default=1, help='input batch size')\n",
    "parser.add_argument('--lr', type=float, default=0.0002)\n",
    "parser.add_argument('--beta1', type=float, default=0.5)  # momentum1 in Adam\n",
    "parser.add_argument('--beta2', type=float, default=0.999)  # momentum2 in Adam\n",
    "parser.add_argument('--lambda_A', type=float, default=10.0)\n",
    "\n",
    "# misc\n",
    "parser.add_argument('--model_path', type=str, default='./SRnDeblur_Neverywhere/models')  # Model Tmp Save\n",
    "parser.add_argument('--sample_path', type=str, default='./SRnDeblur_Neverywhere/results')  # Results\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=100)\n",
    "parser.add_argument('--num_workers', type=int, default=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batchSize=64, beta1=0.5, beta2=0.999, dataroot='./datasets/face_SRnDeblur', lambda_A=10.0, log_step=100, lr=0.0002, model_path='./SRnDeblur_Neverywhere/models', no_flip=False, no_resize_or_crop=True, num_epochs=1001, num_workers=2, sample_path='./SRnDeblur_Neverywhere/results', sample_step=100, which_direction='AtoB')\n"
     ]
    }
   ],
   "source": [
    "# Pre-settings\n",
    "cudnn.benchmark = True\n",
    "global args\n",
    "args = parser.parse_args(['--dataroot','./datasets/face_SRnDeblur','--which_direction','AtoB',\n",
    "                          '--num_epochs','1001','--batchSize','64','--no_resize_or_crop',\n",
    "                          '--log_step','100'])\n",
    "print(args)\n",
    "\n",
    "dataset = ImageFolder(args)\n",
    "\n",
    "data_loader = data.DataLoader(dataset=dataset, batch_size=args.batchSize, shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "if not os.path.exists(args.sample_path):\n",
    "    os.makedirs(args.sample_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1001], BatchStep[100/450]\n",
      "D_loss: 0.0261, G_loss: 20.3309\n",
      "D_RealHR_loss: 0.0178, D_FakeHR_loss: 0.0439, D_RealHRM_loss: 0.0231, D_FakeHRM_loss: 0.0196\n",
      "GAN_loss: 1.9430, G_L1_LR_loss: 0.2119, G_L1_HR_loss: 0.4025, G_L1_HRM_loss: 0.1277, G_vgg_HR_loss: 1.0967 \n",
      "Epoch [1/1001], BatchStep[200/450]\n",
      "D_loss: 0.0158, G_loss: 18.2189\n",
      "D_RealHR_loss: 0.0190, D_FakeHR_loss: 0.0290, D_RealHRM_loss: 0.0056, D_FakeHRM_loss: 0.0095\n",
      "GAN_loss: 1.9812, G_L1_LR_loss: 0.1954, G_L1_HR_loss: 0.3284, G_L1_HRM_loss: 0.0952, G_vgg_HR_loss: 1.0047 \n",
      "Epoch [1/1001], BatchStep[300/450]\n",
      "D_loss: 0.0105, G_loss: 16.8386\n",
      "D_RealHR_loss: 0.0199, D_FakeHR_loss: 0.0101, D_RealHRM_loss: 0.0048, D_FakeHRM_loss: 0.0071\n",
      "GAN_loss: 2.0075, G_L1_LR_loss: 0.1653, G_L1_HR_loss: 0.2966, G_L1_HRM_loss: 0.0894, G_vgg_HR_loss: 0.9317 \n",
      "Epoch [1/1001], BatchStep[400/450]\n",
      "D_loss: 0.0047, G_loss: 15.8431\n",
      "D_RealHR_loss: 0.0040, D_FakeHR_loss: 0.0051, D_RealHRM_loss: 0.0062, D_FakeHRM_loss: 0.0035\n",
      "GAN_loss: 1.9600, G_L1_LR_loss: 0.1605, G_L1_HR_loss: 0.2763, G_L1_HRM_loss: 0.0802, G_vgg_HR_loss: 0.8714 \n",
      "Epoch [2/1001], BatchStep[100/450]\n",
      "D_loss: 0.0053, G_loss: 15.6445\n",
      "D_RealHR_loss: 0.0053, D_FakeHR_loss: 0.0072, D_RealHRM_loss: 0.0053, D_FakeHRM_loss: 0.0034\n",
      "GAN_loss: 1.9041, G_L1_LR_loss: 0.1577, G_L1_HR_loss: 0.2696, G_L1_HRM_loss: 0.0814, G_vgg_HR_loss: 0.8654 \n",
      "Epoch [2/1001], BatchStep[200/450]\n",
      "D_loss: 0.0047, G_loss: 16.9893\n",
      "D_RealHR_loss: 0.0025, D_FakeHR_loss: 0.0027, D_RealHRM_loss: 0.0069, D_FakeHRM_loss: 0.0065\n",
      "GAN_loss: 1.9628, G_L1_LR_loss: 0.1562, G_L1_HR_loss: 0.2968, G_L1_HRM_loss: 0.0945, G_vgg_HR_loss: 0.9552 \n",
      "Epoch [2/1001], BatchStep[300/450]\n",
      "D_loss: 0.0125, G_loss: 15.1792\n",
      "D_RealHR_loss: 0.0178, D_FakeHR_loss: 0.0047, D_RealHRM_loss: 0.0073, D_FakeHRM_loss: 0.0199\n",
      "GAN_loss: 2.0078, G_L1_LR_loss: 0.1443, G_L1_HR_loss: 0.2552, G_L1_HRM_loss: 0.0789, G_vgg_HR_loss: 0.8387 \n",
      "Epoch [2/1001], BatchStep[400/450]\n",
      "D_loss: 0.0031, G_loss: 14.9043\n",
      "D_RealHR_loss: 0.0058, D_FakeHR_loss: 0.0029, D_RealHRM_loss: 0.0018, D_FakeHRM_loss: 0.0021\n",
      "GAN_loss: 1.9923, G_L1_LR_loss: 0.1413, G_L1_HR_loss: 0.2448, G_L1_HRM_loss: 0.0761, G_vgg_HR_loss: 0.8290 \n",
      "Epoch [3/1001], BatchStep[100/450]\n",
      "D_loss: 0.0227, G_loss: 14.7908\n",
      "D_RealHR_loss: 0.0036, D_FakeHR_loss: 0.0020, D_RealHRM_loss: 0.0540, D_FakeHRM_loss: 0.0311\n",
      "GAN_loss: 1.9766, G_L1_LR_loss: 0.1386, G_L1_HR_loss: 0.2512, G_L1_HRM_loss: 0.0762, G_vgg_HR_loss: 0.8154 \n",
      "Epoch [3/1001], BatchStep[200/450]\n",
      "D_loss: 0.0031, G_loss: 15.5056\n",
      "D_RealHR_loss: 0.0011, D_FakeHR_loss: 0.0064, D_RealHRM_loss: 0.0027, D_FakeHRM_loss: 0.0023\n",
      "GAN_loss: 2.0324, G_L1_LR_loss: 0.1493, G_L1_HR_loss: 0.2582, G_L1_HRM_loss: 0.0801, G_vgg_HR_loss: 0.8597 \n",
      "Epoch [3/1001], BatchStep[300/450]\n",
      "D_loss: 0.0062, G_loss: 14.7368\n",
      "D_RealHR_loss: 0.0043, D_FakeHR_loss: 0.0014, D_RealHRM_loss: 0.0074, D_FakeHRM_loss: 0.0116\n",
      "GAN_loss: 2.0324, G_L1_LR_loss: 0.1341, G_L1_HR_loss: 0.2415, G_L1_HRM_loss: 0.0752, G_vgg_HR_loss: 0.8197 \n",
      "Epoch [3/1001], BatchStep[400/450]\n",
      "D_loss: 0.0081, G_loss: 14.5979\n",
      "D_RealHR_loss: 0.0077, D_FakeHR_loss: 0.0046, D_RealHRM_loss: 0.0011, D_FakeHRM_loss: 0.0191\n",
      "GAN_loss: 1.9872, G_L1_LR_loss: 0.1322, G_L1_HR_loss: 0.2371, G_L1_HRM_loss: 0.0718, G_vgg_HR_loss: 0.8201 \n",
      "Epoch [4/1001], BatchStep[100/450]\n",
      "D_loss: 0.0086, G_loss: 14.4138\n",
      "D_RealHR_loss: 0.0220, D_FakeHR_loss: 0.0041, D_RealHRM_loss: 0.0057, D_FakeHRM_loss: 0.0027\n",
      "GAN_loss: 1.9426, G_L1_LR_loss: 0.1409, G_L1_HR_loss: 0.2375, G_L1_HRM_loss: 0.0717, G_vgg_HR_loss: 0.7970 \n",
      "Epoch [4/1001], BatchStep[200/450]\n",
      "D_loss: 0.0015, G_loss: 14.5968\n",
      "D_RealHR_loss: 0.0008, D_FakeHR_loss: 0.0013, D_RealHRM_loss: 0.0032, D_FakeHRM_loss: 0.0009\n",
      "GAN_loss: 2.0071, G_L1_LR_loss: 0.1417, G_L1_HR_loss: 0.2412, G_L1_HRM_loss: 0.0740, G_vgg_HR_loss: 0.8020 \n",
      "Epoch [4/1001], BatchStep[300/450]\n",
      "D_loss: 0.0051, G_loss: 14.1545\n",
      "D_RealHR_loss: 0.0024, D_FakeHR_loss: 0.0006, D_RealHRM_loss: 0.0102, D_FakeHRM_loss: 0.0072\n",
      "GAN_loss: 1.9814, G_L1_LR_loss: 0.1247, G_L1_HR_loss: 0.2273, G_L1_HRM_loss: 0.0706, G_vgg_HR_loss: 0.7947 \n",
      "Epoch [4/1001], BatchStep[400/450]\n",
      "D_loss: 0.0027, G_loss: 14.4987\n",
      "D_RealHR_loss: 0.0015, D_FakeHR_loss: 0.0016, D_RealHRM_loss: 0.0036, D_FakeHRM_loss: 0.0040\n",
      "GAN_loss: 1.9214, G_L1_LR_loss: 0.1394, G_L1_HR_loss: 0.2378, G_L1_HRM_loss: 0.0784, G_vgg_HR_loss: 0.8021 \n",
      "Epoch [5/1001], BatchStep[100/450]\n",
      "D_loss: 0.0028, G_loss: 14.2684\n",
      "D_RealHR_loss: 0.0034, D_FakeHR_loss: 0.0020, D_RealHRM_loss: 0.0034, D_FakeHRM_loss: 0.0022\n",
      "GAN_loss: 2.1056, G_L1_LR_loss: 0.1227, G_L1_HR_loss: 0.2304, G_L1_HRM_loss: 0.0743, G_vgg_HR_loss: 0.7889 \n",
      "Epoch [5/1001], BatchStep[200/450]\n",
      "D_loss: 0.0054, G_loss: 16.4601\n",
      "D_RealHR_loss: 0.0062, D_FakeHR_loss: 0.0010, D_RealHRM_loss: 0.0084, D_FakeHRM_loss: 0.0060\n",
      "GAN_loss: 2.0987, G_L1_LR_loss: 0.1392, G_L1_HR_loss: 0.2901, G_L1_HRM_loss: 0.0922, G_vgg_HR_loss: 0.9147 \n",
      "Epoch [5/1001], BatchStep[300/450]\n",
      "D_loss: 0.0020, G_loss: 14.6345\n",
      "D_RealHR_loss: 0.0018, D_FakeHR_loss: 0.0025, D_RealHRM_loss: 0.0027, D_FakeHRM_loss: 0.0010\n",
      "GAN_loss: 2.0710, G_L1_LR_loss: 0.1340, G_L1_HR_loss: 0.2376, G_L1_HRM_loss: 0.0754, G_vgg_HR_loss: 0.8094 \n",
      "Epoch [5/1001], BatchStep[400/450]\n",
      "D_loss: 0.2464, G_loss: 12.5501\n",
      "D_RealHR_loss: 0.2379, D_FakeHR_loss: 0.2352, D_RealHRM_loss: 0.2932, D_FakeHRM_loss: 0.2193\n",
      "GAN_loss: 0.5533, G_L1_LR_loss: 0.1315, G_L1_HR_loss: 0.2108, G_L1_HRM_loss: 0.0679, G_vgg_HR_loss: 0.7895 \n",
      "Epoch [6/1001], BatchStep[100/450]\n",
      "D_loss: 0.0683, G_loss: 14.0620\n",
      "D_RealHR_loss: 0.0121, D_FakeHR_loss: 0.0231, D_RealHRM_loss: 0.1043, D_FakeHRM_loss: 0.1337\n",
      "GAN_loss: 1.6236, G_L1_LR_loss: 0.1391, G_L1_HR_loss: 0.2212, G_L1_HRM_loss: 0.0732, G_vgg_HR_loss: 0.8104 \n",
      "Epoch [6/1001], BatchStep[200/450]\n",
      "D_loss: 0.0202, G_loss: 14.3389\n",
      "D_RealHR_loss: 0.0073, D_FakeHR_loss: 0.0211, D_RealHRM_loss: 0.0204, D_FakeHRM_loss: 0.0320\n",
      "GAN_loss: 1.9763, G_L1_LR_loss: 0.1327, G_L1_HR_loss: 0.2265, G_L1_HRM_loss: 0.0734, G_vgg_HR_loss: 0.8037 \n",
      "Epoch [6/1001], BatchStep[300/450]\n",
      "D_loss: 0.0095, G_loss: 15.9203\n",
      "D_RealHR_loss: 0.0036, D_FakeHR_loss: 0.0097, D_RealHRM_loss: 0.0092, D_FakeHRM_loss: 0.0154\n",
      "GAN_loss: 2.0312, G_L1_LR_loss: 0.1366, G_L1_HR_loss: 0.2589, G_L1_HRM_loss: 0.0822, G_vgg_HR_loss: 0.9112 \n",
      "Epoch [6/1001], BatchStep[400/450]\n",
      "D_loss: 0.0082, G_loss: 14.2467\n",
      "D_RealHR_loss: 0.0084, D_FakeHR_loss: 0.0075, D_RealHRM_loss: 0.0095, D_FakeHRM_loss: 0.0074\n",
      "GAN_loss: 1.9647, G_L1_LR_loss: 0.1399, G_L1_HR_loss: 0.2258, G_L1_HRM_loss: 0.0714, G_vgg_HR_loss: 0.7910 \n",
      "Epoch [7/1001], BatchStep[100/450]\n",
      "D_loss: 0.0056, G_loss: 14.8504\n",
      "D_RealHR_loss: 0.0038, D_FakeHR_loss: 0.0085, D_RealHRM_loss: 0.0061, D_FakeHRM_loss: 0.0040\n",
      "GAN_loss: 2.0989, G_L1_LR_loss: 0.1554, G_L1_HR_loss: 0.2386, G_L1_HRM_loss: 0.0743, G_vgg_HR_loss: 0.8069 \n",
      "Epoch [7/1001], BatchStep[200/450]\n",
      "D_loss: 0.0039, G_loss: 14.3990\n",
      "D_RealHR_loss: 0.0032, D_FakeHR_loss: 0.0020, D_RealHRM_loss: 0.0051, D_FakeHRM_loss: 0.0053\n",
      "GAN_loss: 1.9630, G_L1_LR_loss: 0.1403, G_L1_HR_loss: 0.2272, G_L1_HRM_loss: 0.0736, G_vgg_HR_loss: 0.8025 \n",
      "Epoch [7/1001], BatchStep[300/450]\n",
      "D_loss: 0.0174, G_loss: 14.2596\n",
      "D_RealHR_loss: 0.0018, D_FakeHR_loss: 0.0027, D_RealHRM_loss: 0.0366, D_FakeHRM_loss: 0.0282\n",
      "GAN_loss: 2.0197, G_L1_LR_loss: 0.1326, G_L1_HR_loss: 0.2256, G_L1_HRM_loss: 0.0747, G_vgg_HR_loss: 0.7911 \n",
      "Epoch [7/1001], BatchStep[400/450]\n",
      "D_loss: 0.0059, G_loss: 14.6571\n",
      "D_RealHR_loss: 0.0097, D_FakeHR_loss: 0.0035, D_RealHRM_loss: 0.0042, D_FakeHRM_loss: 0.0062\n",
      "GAN_loss: 1.9823, G_L1_LR_loss: 0.1496, G_L1_HR_loss: 0.2303, G_L1_HRM_loss: 0.0734, G_vgg_HR_loss: 0.8141 \n",
      "Epoch [8/1001], BatchStep[100/450]\n",
      "D_loss: 0.0124, G_loss: 14.1703\n",
      "D_RealHR_loss: 0.0049, D_FakeHR_loss: 0.0080, D_RealHRM_loss: 0.0142, D_FakeHRM_loss: 0.0226\n",
      "GAN_loss: 2.0281, G_L1_LR_loss: 0.1290, G_L1_HR_loss: 0.2208, G_L1_HRM_loss: 0.0692, G_vgg_HR_loss: 0.7952 \n",
      "Epoch [8/1001], BatchStep[200/450]\n",
      "D_loss: 0.0022, G_loss: 14.7662\n",
      "D_RealHR_loss: 0.0018, D_FakeHR_loss: 0.0030, D_RealHRM_loss: 0.0024, D_FakeHRM_loss: 0.0017\n",
      "GAN_loss: 1.9884, G_L1_LR_loss: 0.1280, G_L1_HR_loss: 0.2289, G_L1_HRM_loss: 0.0762, G_vgg_HR_loss: 0.8447 \n",
      "Epoch [8/1001], BatchStep[300/450]\n",
      "D_loss: 0.0036, G_loss: 14.5883\n",
      "D_RealHR_loss: 0.0013, D_FakeHR_loss: 0.0019, D_RealHRM_loss: 0.0072, D_FakeHRM_loss: 0.0040\n",
      "GAN_loss: 1.9670, G_L1_LR_loss: 0.1589, G_L1_HR_loss: 0.2354, G_L1_HRM_loss: 0.0770, G_vgg_HR_loss: 0.7908 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1001], BatchStep[400/450]\n",
      "D_loss: 0.0135, G_loss: 14.8094\n",
      "D_RealHR_loss: 0.0008, D_FakeHR_loss: 0.0010, D_RealHRM_loss: 0.0243, D_FakeHRM_loss: 0.0280\n",
      "GAN_loss: 1.9914, G_L1_LR_loss: 0.1275, G_L1_HR_loss: 0.2273, G_L1_HRM_loss: 0.0771, G_vgg_HR_loss: 0.8499 \n",
      "Epoch [9/1001], BatchStep[100/450]\n",
      "D_loss: 0.0050, G_loss: 14.1838\n",
      "D_RealHR_loss: 0.0019, D_FakeHR_loss: 0.0016, D_RealHRM_loss: 0.0051, D_FakeHRM_loss: 0.0112\n",
      "GAN_loss: 2.0431, G_L1_LR_loss: 0.1398, G_L1_HR_loss: 0.2178, G_L1_HRM_loss: 0.0700, G_vgg_HR_loss: 0.7865 \n",
      "Epoch [9/1001], BatchStep[200/450]\n",
      "D_loss: 0.0320, G_loss: 15.9926\n",
      "D_RealHR_loss: 0.0042, D_FakeHR_loss: 0.0014, D_RealHRM_loss: 0.0849, D_FakeHRM_loss: 0.0374\n",
      "GAN_loss: 2.0510, G_L1_LR_loss: 0.1465, G_L1_HR_loss: 0.2733, G_L1_HRM_loss: 0.0967, G_vgg_HR_loss: 0.8776 \n",
      "Epoch [9/1001], BatchStep[300/450]\n",
      "D_loss: 0.0061, G_loss: 14.3355\n",
      "D_RealHR_loss: 0.0024, D_FakeHR_loss: 0.0030, D_RealHRM_loss: 0.0082, D_FakeHRM_loss: 0.0108\n",
      "GAN_loss: 2.0035, G_L1_LR_loss: 0.1219, G_L1_HR_loss: 0.2267, G_L1_HRM_loss: 0.0784, G_vgg_HR_loss: 0.8062 \n",
      "Epoch [9/1001], BatchStep[400/450]\n",
      "D_loss: 0.0123, G_loss: 16.7606\n",
      "D_RealHR_loss: 0.0011, D_FakeHR_loss: 0.0068, D_RealHRM_loss: 0.0141, D_FakeHRM_loss: 0.0270\n",
      "GAN_loss: 2.0429, G_L1_LR_loss: 0.1324, G_L1_HR_loss: 0.2688, G_L1_HRM_loss: 0.0912, G_vgg_HR_loss: 0.9794 \n",
      "Epoch [10/1001], BatchStep[100/450]\n",
      "D_loss: 0.0110, G_loss: 13.5872\n",
      "D_RealHR_loss: 0.0015, D_FakeHR_loss: 0.0007, D_RealHRM_loss: 0.0222, D_FakeHRM_loss: 0.0195\n",
      "GAN_loss: 1.9444, G_L1_LR_loss: 0.1190, G_L1_HR_loss: 0.2083, G_L1_HRM_loss: 0.0670, G_vgg_HR_loss: 0.7701 \n",
      "Epoch [10/1001], BatchStep[200/450]\n",
      "D_loss: 0.0107, G_loss: 13.7824\n",
      "D_RealHR_loss: 0.0015, D_FakeHR_loss: 0.0009, D_RealHRM_loss: 0.0244, D_FakeHRM_loss: 0.0159\n",
      "GAN_loss: 2.0189, G_L1_LR_loss: 0.1155, G_L1_HR_loss: 0.2096, G_L1_HRM_loss: 0.0704, G_vgg_HR_loss: 0.7809 \n",
      "Epoch [10/1001], BatchStep[300/450]\n",
      "D_loss: 0.0105, G_loss: 13.7264\n",
      "D_RealHR_loss: 0.0025, D_FakeHR_loss: 0.0018, D_RealHRM_loss: 0.0204, D_FakeHRM_loss: 0.0176\n",
      "GAN_loss: 2.0415, G_L1_LR_loss: 0.1231, G_L1_HR_loss: 0.2081, G_L1_HRM_loss: 0.0658, G_vgg_HR_loss: 0.7716 \n",
      "Epoch [10/1001], BatchStep[400/450]\n",
      "D_loss: 0.0083, G_loss: 14.4907\n",
      "D_RealHR_loss: 0.0019, D_FakeHR_loss: 0.0024, D_RealHRM_loss: 0.0109, D_FakeHRM_loss: 0.0182\n",
      "GAN_loss: 1.9293, G_L1_LR_loss: 0.1199, G_L1_HR_loss: 0.2281, G_L1_HRM_loss: 0.0805, G_vgg_HR_loss: 0.8276 \n",
      "Epoch [11/1001], BatchStep[100/450]\n",
      "D_loss: 0.0168, G_loss: 15.5141\n",
      "D_RealHR_loss: 0.0127, D_FakeHR_loss: 0.0049, D_RealHRM_loss: 0.0256, D_FakeHRM_loss: 0.0241\n",
      "GAN_loss: 2.1273, G_L1_LR_loss: 0.1284, G_L1_HR_loss: 0.2501, G_L1_HRM_loss: 0.0880, G_vgg_HR_loss: 0.8722 \n",
      "Epoch [11/1001], BatchStep[200/450]\n",
      "D_loss: 0.0047, G_loss: 13.4488\n",
      "D_RealHR_loss: 0.0010, D_FakeHR_loss: 0.0016, D_RealHRM_loss: 0.0100, D_FakeHRM_loss: 0.0061\n",
      "GAN_loss: 2.0026, G_L1_LR_loss: 0.1160, G_L1_HR_loss: 0.1995, G_L1_HRM_loss: 0.0684, G_vgg_HR_loss: 0.7608 \n",
      "Epoch [11/1001], BatchStep[300/450]\n",
      "D_loss: 0.0020, G_loss: 13.4394\n",
      "D_RealHR_loss: 0.0013, D_FakeHR_loss: 0.0008, D_RealHRM_loss: 0.0026, D_FakeHRM_loss: 0.0033\n",
      "GAN_loss: 2.0161, G_L1_LR_loss: 0.1195, G_L1_HR_loss: 0.2034, G_L1_HRM_loss: 0.0654, G_vgg_HR_loss: 0.7539 \n",
      "Epoch [11/1001], BatchStep[400/450]\n",
      "D_loss: 0.0106, G_loss: 15.5392\n",
      "D_RealHR_loss: 0.0149, D_FakeHR_loss: 0.0055, D_RealHRM_loss: 0.0151, D_FakeHRM_loss: 0.0071\n",
      "GAN_loss: 1.9270, G_L1_LR_loss: 0.1189, G_L1_HR_loss: 0.2201, G_L1_HRM_loss: 0.0735, G_vgg_HR_loss: 0.9488 \n",
      "Epoch [12/1001], BatchStep[100/450]\n",
      "D_loss: 0.0228, G_loss: 14.0588\n",
      "D_RealHR_loss: 0.0019, D_FakeHR_loss: 0.0007, D_RealHRM_loss: 0.0475, D_FakeHRM_loss: 0.0412\n",
      "GAN_loss: 2.0568, G_L1_LR_loss: 0.1221, G_L1_HR_loss: 0.2215, G_L1_HRM_loss: 0.0772, G_vgg_HR_loss: 0.7794 \n",
      "Epoch [12/1001], BatchStep[200/450]\n",
      "D_loss: 0.0093, G_loss: 13.2561\n",
      "D_RealHR_loss: 0.0089, D_FakeHR_loss: 0.0014, D_RealHRM_loss: 0.0143, D_FakeHRM_loss: 0.0127\n",
      "GAN_loss: 1.9658, G_L1_LR_loss: 0.1113, G_L1_HR_loss: 0.2005, G_L1_HRM_loss: 0.0683, G_vgg_HR_loss: 0.7489 \n",
      "Epoch [12/1001], BatchStep[300/450]\n",
      "D_loss: 0.0103, G_loss: 13.3879\n",
      "D_RealHR_loss: 0.0101, D_FakeHR_loss: 0.0021, D_RealHRM_loss: 0.0115, D_FakeHRM_loss: 0.0177\n",
      "GAN_loss: 2.0084, G_L1_LR_loss: 0.1202, G_L1_HR_loss: 0.2028, G_L1_HRM_loss: 0.0674, G_vgg_HR_loss: 0.7475 \n",
      "Epoch [12/1001], BatchStep[400/450]\n",
      "D_loss: 0.1042, G_loss: 12.3458\n",
      "D_RealHR_loss: 0.1885, D_FakeHR_loss: 0.2076, D_RealHRM_loss: 0.0118, D_FakeHRM_loss: 0.0091\n",
      "GAN_loss: 1.2497, G_L1_LR_loss: 0.1149, G_L1_HR_loss: 0.1769, G_L1_HRM_loss: 0.0556, G_vgg_HR_loss: 0.7622 \n",
      "Epoch [13/1001], BatchStep[100/450]\n",
      "D_loss: 0.0127, G_loss: 13.0610\n",
      "D_RealHR_loss: 0.0150, D_FakeHR_loss: 0.0254, D_RealHRM_loss: 0.0041, D_FakeHRM_loss: 0.0066\n",
      "GAN_loss: 1.8623, G_L1_LR_loss: 0.1237, G_L1_HR_loss: 0.1792, G_L1_HRM_loss: 0.0584, G_vgg_HR_loss: 0.7586 \n",
      "Epoch [13/1001], BatchStep[200/450]\n",
      "D_loss: 0.0058, G_loss: 13.2151\n",
      "D_RealHR_loss: 0.0102, D_FakeHR_loss: 0.0093, D_RealHRM_loss: 0.0014, D_FakeHRM_loss: 0.0022\n",
      "GAN_loss: 2.0061, G_L1_LR_loss: 0.1222, G_L1_HR_loss: 0.1974, G_L1_HRM_loss: 0.0594, G_vgg_HR_loss: 0.7419 \n",
      "Epoch [13/1001], BatchStep[300/450]\n",
      "D_loss: 0.0044, G_loss: 12.7825\n",
      "D_RealHR_loss: 0.0043, D_FakeHR_loss: 0.0034, D_RealHRM_loss: 0.0051, D_FakeHRM_loss: 0.0048\n",
      "GAN_loss: 2.0239, G_L1_LR_loss: 0.1109, G_L1_HR_loss: 0.1722, G_L1_HRM_loss: 0.0548, G_vgg_HR_loss: 0.7380 \n",
      "Epoch [13/1001], BatchStep[400/450]\n",
      "D_loss: 0.0028, G_loss: 12.7381\n",
      "D_RealHR_loss: 0.0036, D_FakeHR_loss: 0.0043, D_RealHRM_loss: 0.0012, D_FakeHRM_loss: 0.0020\n",
      "GAN_loss: 1.9722, G_L1_LR_loss: 0.1107, G_L1_HR_loss: 0.1761, G_L1_HRM_loss: 0.0560, G_vgg_HR_loss: 0.7339 \n",
      "Epoch [14/1001], BatchStep[100/450]\n",
      "D_loss: 0.0179, G_loss: 12.8742\n",
      "D_RealHR_loss: 0.0315, D_FakeHR_loss: 0.0219, D_RealHRM_loss: 0.0114, D_FakeHRM_loss: 0.0069\n",
      "GAN_loss: 1.9166, G_L1_LR_loss: 0.1231, G_L1_HR_loss: 0.1771, G_L1_HRM_loss: 0.0574, G_vgg_HR_loss: 0.7382 \n",
      "Epoch [14/1001], BatchStep[200/450]\n",
      "D_loss: 0.0050, G_loss: 12.8928\n",
      "D_RealHR_loss: 0.0035, D_FakeHR_loss: 0.0021, D_RealHRM_loss: 0.0081, D_FakeHRM_loss: 0.0064\n",
      "GAN_loss: 2.0135, G_L1_LR_loss: 0.1134, G_L1_HR_loss: 0.1739, G_L1_HRM_loss: 0.0534, G_vgg_HR_loss: 0.7472 \n",
      "Epoch [14/1001], BatchStep[300/450]\n",
      "D_loss: 0.0082, G_loss: 12.6471\n",
      "D_RealHR_loss: 0.0023, D_FakeHR_loss: 0.0028, D_RealHRM_loss: 0.0155, D_FakeHRM_loss: 0.0121\n",
      "GAN_loss: 1.9362, G_L1_LR_loss: 0.1111, G_L1_HR_loss: 0.1713, G_L1_HRM_loss: 0.0523, G_vgg_HR_loss: 0.7365 \n",
      "Epoch [14/1001], BatchStep[400/450]\n",
      "D_loss: 0.0036, G_loss: 12.8534\n",
      "D_RealHR_loss: 0.0023, D_FakeHR_loss: 0.0015, D_RealHRM_loss: 0.0048, D_FakeHRM_loss: 0.0058\n",
      "GAN_loss: 2.0204, G_L1_LR_loss: 0.1170, G_L1_HR_loss: 0.1747, G_L1_HRM_loss: 0.0534, G_vgg_HR_loss: 0.7381 \n",
      "Epoch [15/1001], BatchStep[100/450]\n",
      "D_loss: 0.0059, G_loss: 12.9920\n",
      "D_RealHR_loss: 0.0033, D_FakeHR_loss: 0.0019, D_RealHRM_loss: 0.0107, D_FakeHRM_loss: 0.0076\n",
      "GAN_loss: 1.9986, G_L1_LR_loss: 0.1320, G_L1_HR_loss: 0.1780, G_L1_HRM_loss: 0.0576, G_vgg_HR_loss: 0.7318 \n",
      "Epoch [15/1001], BatchStep[200/450]\n",
      "D_loss: 0.0072, G_loss: 15.0761\n",
      "D_RealHR_loss: 0.0120, D_FakeHR_loss: 0.0042, D_RealHRM_loss: 0.0031, D_FakeHRM_loss: 0.0094\n",
      "GAN_loss: 1.9802, G_L1_LR_loss: 0.1280, G_L1_HR_loss: 0.2275, G_L1_HRM_loss: 0.0803, G_vgg_HR_loss: 0.8738 \n",
      "Epoch [15/1001], BatchStep[300/450]\n",
      "D_loss: 0.0015, G_loss: 12.7571\n",
      "D_RealHR_loss: 0.0031, D_FakeHR_loss: 0.0018, D_RealHRM_loss: 0.0006, D_FakeHRM_loss: 0.0006\n",
      "GAN_loss: 2.0548, G_L1_LR_loss: 0.1131, G_L1_HR_loss: 0.1737, G_L1_HRM_loss: 0.0557, G_vgg_HR_loss: 0.7277 \n",
      "Epoch [15/1001], BatchStep[400/450]\n",
      "D_loss: 0.0089, G_loss: 12.7431\n",
      "D_RealHR_loss: 0.0015, D_FakeHR_loss: 0.0013, D_RealHRM_loss: 0.0184, D_FakeHRM_loss: 0.0144\n",
      "GAN_loss: 2.0897, G_L1_LR_loss: 0.1126, G_L1_HR_loss: 0.1677, G_L1_HRM_loss: 0.0552, G_vgg_HR_loss: 0.7298 \n",
      "Epoch [16/1001], BatchStep[100/450]\n",
      "D_loss: 0.0015, G_loss: 12.8315\n",
      "D_RealHR_loss: 0.0008, D_FakeHR_loss: 0.0012, D_RealHRM_loss: 0.0020, D_FakeHRM_loss: 0.0020\n",
      "GAN_loss: 2.0065, G_L1_LR_loss: 0.1147, G_L1_HR_loss: 0.1730, G_L1_HRM_loss: 0.0563, G_vgg_HR_loss: 0.7386 \n",
      "Epoch [16/1001], BatchStep[200/450]\n",
      "D_loss: 0.0059, G_loss: 12.7374\n",
      "D_RealHR_loss: 0.0011, D_FakeHR_loss: 0.0010, D_RealHRM_loss: 0.0102, D_FakeHRM_loss: 0.0112\n",
      "GAN_loss: 1.9431, G_L1_LR_loss: 0.1138, G_L1_HR_loss: 0.1778, G_L1_HRM_loss: 0.0552, G_vgg_HR_loss: 0.7327 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/1001], BatchStep[300/450]\n",
      "D_loss: 0.0028, G_loss: 12.7996\n",
      "D_RealHR_loss: 0.0049, D_FakeHR_loss: 0.0030, D_RealHRM_loss: 0.0021, D_FakeHRM_loss: 0.0014\n",
      "GAN_loss: 2.0574, G_L1_LR_loss: 0.1065, G_L1_HR_loss: 0.1685, G_L1_HRM_loss: 0.0524, G_vgg_HR_loss: 0.7468 \n",
      "Epoch [16/1001], BatchStep[400/450]\n",
      "D_loss: 0.0022, G_loss: 12.4205\n",
      "D_RealHR_loss: 0.0042, D_FakeHR_loss: 0.0021, D_RealHRM_loss: 0.0016, D_FakeHRM_loss: 0.0009\n",
      "GAN_loss: 1.9652, G_L1_LR_loss: 0.1049, G_L1_HR_loss: 0.1654, G_L1_HRM_loss: 0.0517, G_vgg_HR_loss: 0.7235 \n",
      "Epoch [17/1001], BatchStep[100/450]\n",
      "D_loss: 0.0026, G_loss: 12.4867\n",
      "D_RealHR_loss: 0.0014, D_FakeHR_loss: 0.0010, D_RealHRM_loss: 0.0040, D_FakeHRM_loss: 0.0041\n",
      "GAN_loss: 2.0205, G_L1_LR_loss: 0.1084, G_L1_HR_loss: 0.1667, G_L1_HRM_loss: 0.0513, G_vgg_HR_loss: 0.7203 \n",
      "Epoch [17/1001], BatchStep[200/450]\n",
      "D_loss: 0.0046, G_loss: 13.5733\n",
      "D_RealHR_loss: 0.0010, D_FakeHR_loss: 0.0013, D_RealHRM_loss: 0.0087, D_FakeHRM_loss: 0.0074\n",
      "GAN_loss: 2.0682, G_L1_LR_loss: 0.1186, G_L1_HR_loss: 0.1944, G_L1_HRM_loss: 0.0644, G_vgg_HR_loss: 0.7731 \n",
      "Epoch [17/1001], BatchStep[300/450]\n",
      "D_loss: 0.0043, G_loss: 12.8065\n",
      "D_RealHR_loss: 0.0015, D_FakeHR_loss: 0.0007, D_RealHRM_loss: 0.0061, D_FakeHRM_loss: 0.0091\n",
      "GAN_loss: 2.0422, G_L1_LR_loss: 0.1207, G_L1_HR_loss: 0.1710, G_L1_HRM_loss: 0.0552, G_vgg_HR_loss: 0.7296 \n",
      "Epoch [17/1001], BatchStep[400/450]\n",
      "D_loss: 0.0019, G_loss: 12.4312\n",
      "D_RealHR_loss: 0.0034, D_FakeHR_loss: 0.0020, D_RealHRM_loss: 0.0009, D_FakeHRM_loss: 0.0012\n",
      "GAN_loss: 2.0510, G_L1_LR_loss: 0.1057, G_L1_HR_loss: 0.1712, G_L1_HRM_loss: 0.0520, G_vgg_HR_loss: 0.7091 \n",
      "Epoch [18/1001], BatchStep[100/450]\n",
      "D_loss: 0.0012, G_loss: 12.2718\n",
      "D_RealHR_loss: 0.0023, D_FakeHR_loss: 0.0006, D_RealHRM_loss: 0.0009, D_FakeHRM_loss: 0.0010\n",
      "GAN_loss: 1.9602, G_L1_LR_loss: 0.1048, G_L1_HR_loss: 0.1677, G_L1_HRM_loss: 0.0523, G_vgg_HR_loss: 0.7064 \n",
      "Epoch [18/1001], BatchStep[200/450]\n",
      "D_loss: 0.0009, G_loss: 12.3173\n",
      "D_RealHR_loss: 0.0017, D_FakeHR_loss: 0.0008, D_RealHRM_loss: 0.0006, D_FakeHRM_loss: 0.0004\n",
      "GAN_loss: 1.9507, G_L1_LR_loss: 0.1053, G_L1_HR_loss: 0.1625, G_L1_HRM_loss: 0.0490, G_vgg_HR_loss: 0.7200 \n",
      "Epoch [18/1001], BatchStep[300/450]\n",
      "D_loss: 0.0071, G_loss: 12.3592\n",
      "D_RealHR_loss: 0.0038, D_FakeHR_loss: 0.0033, D_RealHRM_loss: 0.0130, D_FakeHRM_loss: 0.0085\n",
      "GAN_loss: 1.9851, G_L1_LR_loss: 0.1069, G_L1_HR_loss: 0.1703, G_L1_HRM_loss: 0.0508, G_vgg_HR_loss: 0.7094 \n",
      "Epoch [18/1001], BatchStep[400/450]\n",
      "D_loss: 2.3751, G_loss: 13.1175\n",
      "D_RealHR_loss: 5.5690, D_FakeHR_loss: 3.9282, D_RealHRM_loss: 0.0019, D_FakeHRM_loss: 0.0014\n",
      "GAN_loss: 2.7852, G_L1_LR_loss: 0.1063, G_L1_HR_loss: 0.1683, G_L1_HRM_loss: 0.0528, G_vgg_HR_loss: 0.7058 \n",
      "Epoch [19/1001], BatchStep[100/450]\n",
      "D_loss: 0.0197, G_loss: 12.8699\n",
      "D_RealHR_loss: 0.0157, D_FakeHR_loss: 0.0075, D_RealHRM_loss: 0.0323, D_FakeHRM_loss: 0.0233\n",
      "GAN_loss: 1.8989, G_L1_LR_loss: 0.1213, G_L1_HR_loss: 0.1796, G_L1_HRM_loss: 0.0563, G_vgg_HR_loss: 0.7398 \n",
      "Epoch [19/1001], BatchStep[200/450]\n",
      "D_loss: 0.0036, G_loss: 13.4840\n",
      "D_RealHR_loss: 0.0075, D_FakeHR_loss: 0.0022, D_RealHRM_loss: 0.0033, D_FakeHRM_loss: 0.0013\n",
      "GAN_loss: 2.0798, G_L1_LR_loss: 0.1529, G_L1_HR_loss: 0.2018, G_L1_HRM_loss: 0.0639, G_vgg_HR_loss: 0.7218 \n",
      "Epoch [19/1001], BatchStep[300/450]\n",
      "D_loss: 0.2516, G_loss: 10.4843\n",
      "D_RealHR_loss: 0.2700, D_FakeHR_loss: 0.2283, D_RealHRM_loss: 0.2744, D_FakeHRM_loss: 0.2336\n",
      "GAN_loss: 0.5385, G_L1_LR_loss: 0.1056, G_L1_HR_loss: 0.1361, G_L1_HRM_loss: 0.0475, G_vgg_HR_loss: 0.7054 \n",
      "Epoch [19/1001], BatchStep[400/450]\n",
      "D_loss: 0.1996, G_loss: 12.0036\n",
      "D_RealHR_loss: 0.2278, D_FakeHR_loss: 0.2548, D_RealHRM_loss: 0.1531, D_FakeHRM_loss: 0.1627\n",
      "GAN_loss: 0.7115, G_L1_LR_loss: 0.1052, G_L1_HR_loss: 0.1587, G_L1_HRM_loss: 0.0557, G_vgg_HR_loss: 0.8096 \n",
      "Epoch [20/1001], BatchStep[100/450]\n",
      "D_loss: 0.1530, G_loss: 11.5219\n",
      "D_RealHR_loss: 0.1062, D_FakeHR_loss: 0.4368, D_RealHRM_loss: 0.0444, D_FakeHRM_loss: 0.0248\n",
      "GAN_loss: 1.3471, G_L1_LR_loss: 0.1127, G_L1_HR_loss: 0.1354, G_L1_HRM_loss: 0.0470, G_vgg_HR_loss: 0.7223 \n",
      "Epoch [20/1001], BatchStep[200/450]\n",
      "D_loss: 0.1312, G_loss: 11.3687\n",
      "D_RealHR_loss: 0.3132, D_FakeHR_loss: 0.1812, D_RealHRM_loss: 0.0226, D_FakeHRM_loss: 0.0078\n",
      "GAN_loss: 1.2349, G_L1_LR_loss: 0.1126, G_L1_HR_loss: 0.1386, G_L1_HRM_loss: 0.0472, G_vgg_HR_loss: 0.7149 \n",
      "Epoch [20/1001], BatchStep[300/450]\n",
      "D_loss: 0.1231, G_loss: 11.3645\n",
      "D_RealHR_loss: 0.1959, D_FakeHR_loss: 0.2861, D_RealHRM_loss: 0.0066, D_FakeHRM_loss: 0.0036\n",
      "GAN_loss: 1.2953, G_L1_LR_loss: 0.1155, G_L1_HR_loss: 0.1390, G_L1_HRM_loss: 0.0467, G_vgg_HR_loss: 0.7057 \n",
      "Epoch [20/1001], BatchStep[400/450]\n",
      "D_loss: 0.1203, G_loss: 11.5045\n",
      "D_RealHR_loss: 0.2240, D_FakeHR_loss: 0.2449, D_RealHRM_loss: 0.0080, D_FakeHRM_loss: 0.0042\n",
      "GAN_loss: 1.3658, G_L1_LR_loss: 0.1091, G_L1_HR_loss: 0.1362, G_L1_HRM_loss: 0.0462, G_vgg_HR_loss: 0.7223 \n",
      "Epoch [21/1001], BatchStep[100/450]\n",
      "D_loss: 0.0951, G_loss: 12.7223\n",
      "D_RealHR_loss: 0.1719, D_FakeHR_loss: 0.1983, D_RealHRM_loss: 0.0055, D_FakeHRM_loss: 0.0047\n",
      "GAN_loss: 1.4053, G_L1_LR_loss: 0.1194, G_L1_HR_loss: 0.1593, G_L1_HRM_loss: 0.0552, G_vgg_HR_loss: 0.7978 \n",
      "Epoch [21/1001], BatchStep[200/450]\n",
      "D_loss: 0.1143, G_loss: 11.4253\n",
      "D_RealHR_loss: 0.3054, D_FakeHR_loss: 0.1375, D_RealHRM_loss: 0.0110, D_FakeHRM_loss: 0.0032\n",
      "GAN_loss: 1.2037, G_L1_LR_loss: 0.1128, G_L1_HR_loss: 0.1369, G_L1_HRM_loss: 0.0455, G_vgg_HR_loss: 0.7269 \n",
      "Epoch [21/1001], BatchStep[300/450]\n",
      "D_loss: 0.1489, G_loss: 11.5236\n",
      "D_RealHR_loss: 0.2364, D_FakeHR_loss: 0.3557, D_RealHRM_loss: 0.0025, D_FakeHRM_loss: 0.0011\n",
      "GAN_loss: 1.3347, G_L1_LR_loss: 0.1119, G_L1_HR_loss: 0.1427, G_L1_HRM_loss: 0.0521, G_vgg_HR_loss: 0.7122 \n",
      "Epoch [21/1001], BatchStep[400/450]\n",
      "D_loss: 0.0989, G_loss: 11.3643\n",
      "D_RealHR_loss: 0.1486, D_FakeHR_loss: 0.2383, D_RealHRM_loss: 0.0032, D_FakeHRM_loss: 0.0056\n",
      "GAN_loss: 1.3635, G_L1_LR_loss: 0.1097, G_L1_HR_loss: 0.1362, G_L1_HRM_loss: 0.0470, G_vgg_HR_loss: 0.7071 \n",
      "Epoch [22/1001], BatchStep[100/450]\n",
      "D_loss: 0.1000, G_loss: 11.8507\n",
      "D_RealHR_loss: 0.1137, D_FakeHR_loss: 0.2801, D_RealHRM_loss: 0.0031, D_FakeHRM_loss: 0.0032\n",
      "GAN_loss: 1.5745, G_L1_LR_loss: 0.1101, G_L1_HR_loss: 0.1382, G_L1_HRM_loss: 0.0473, G_vgg_HR_loss: 0.7320 \n",
      "Epoch [22/1001], BatchStep[200/450]\n",
      "D_loss: 0.1842, G_loss: 11.4342\n",
      "D_RealHR_loss: 0.0225, D_FakeHR_loss: 0.7077, D_RealHRM_loss: 0.0038, D_FakeHRM_loss: 0.0027\n",
      "GAN_loss: 1.5548, G_L1_LR_loss: 0.1059, G_L1_HR_loss: 0.1326, G_L1_HRM_loss: 0.0445, G_vgg_HR_loss: 0.7048 \n",
      "Epoch [22/1001], BatchStep[300/450]\n",
      "D_loss: 0.0846, G_loss: 11.7943\n",
      "D_RealHR_loss: 0.1554, D_FakeHR_loss: 0.1797, D_RealHRM_loss: 0.0019, D_FakeHRM_loss: 0.0014\n",
      "GAN_loss: 1.6931, G_L1_LR_loss: 0.1115, G_L1_HR_loss: 0.1359, G_L1_HRM_loss: 0.0454, G_vgg_HR_loss: 0.7174 \n",
      "Epoch [22/1001], BatchStep[400/450]\n",
      "D_loss: 0.0997, G_loss: 11.4856\n",
      "D_RealHR_loss: 0.1995, D_FakeHR_loss: 0.1925, D_RealHRM_loss: 0.0047, D_FakeHRM_loss: 0.0020\n",
      "GAN_loss: 1.3626, G_L1_LR_loss: 0.1175, G_L1_HR_loss: 0.1413, G_L1_HRM_loss: 0.0467, G_vgg_HR_loss: 0.7068 \n",
      "Epoch [23/1001], BatchStep[100/450]\n",
      "D_loss: 0.0594, G_loss: 11.8916\n",
      "D_RealHR_loss: 0.1592, D_FakeHR_loss: 0.0745, D_RealHRM_loss: 0.0023, D_FakeHRM_loss: 0.0016\n",
      "GAN_loss: 1.7050, G_L1_LR_loss: 0.1135, G_L1_HR_loss: 0.1389, G_L1_HRM_loss: 0.0474, G_vgg_HR_loss: 0.7189 \n",
      "Epoch [23/1001], BatchStep[200/450]\n",
      "D_loss: 0.0898, G_loss: 11.7807\n",
      "D_RealHR_loss: 0.3207, D_FakeHR_loss: 0.0325, D_RealHRM_loss: 0.0050, D_FakeHRM_loss: 0.0011\n",
      "GAN_loss: 1.5078, G_L1_LR_loss: 0.1116, G_L1_HR_loss: 0.1416, G_L1_HRM_loss: 0.0447, G_vgg_HR_loss: 0.7294 \n",
      "Epoch [23/1001], BatchStep[300/450]\n",
      "D_loss: 0.0332, G_loss: 11.7757\n",
      "D_RealHR_loss: 0.0490, D_FakeHR_loss: 0.0769, D_RealHRM_loss: 0.0042, D_FakeHRM_loss: 0.0027\n",
      "GAN_loss: 1.6730, G_L1_LR_loss: 0.1131, G_L1_HR_loss: 0.1388, G_L1_HRM_loss: 0.0471, G_vgg_HR_loss: 0.7112 \n",
      "Epoch [23/1001], BatchStep[400/450]\n",
      "D_loss: 0.0317, G_loss: 11.8988\n",
      "D_RealHR_loss: 0.0855, D_FakeHR_loss: 0.0296, D_RealHRM_loss: 0.0058, D_FakeHRM_loss: 0.0058\n",
      "GAN_loss: 1.5539, G_L1_LR_loss: 0.1231, G_L1_HR_loss: 0.1474, G_L1_HRM_loss: 0.0507, G_vgg_HR_loss: 0.7133 \n",
      "Epoch [24/1001], BatchStep[100/450]\n",
      "D_loss: 0.1203, G_loss: 11.9924\n",
      "D_RealHR_loss: 0.0326, D_FakeHR_loss: 0.4403, D_RealHRM_loss: 0.0044, D_FakeHRM_loss: 0.0037\n",
      "GAN_loss: 1.5260, G_L1_LR_loss: 0.1261, G_L1_HR_loss: 0.1511, G_L1_HRM_loss: 0.0530, G_vgg_HR_loss: 0.7165 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/1001], BatchStep[200/450]\n",
      "D_loss: 0.1693, G_loss: 11.7508\n",
      "D_RealHR_loss: 0.0291, D_FakeHR_loss: 0.6438, D_RealHRM_loss: 0.0025, D_FakeHRM_loss: 0.0018\n",
      "GAN_loss: 1.7314, G_L1_LR_loss: 0.1104, G_L1_HR_loss: 0.1419, G_L1_HRM_loss: 0.0476, G_vgg_HR_loss: 0.7020 \n",
      "Epoch [24/1001], BatchStep[300/450]\n",
      "D_loss: 0.0711, G_loss: 11.6905\n",
      "D_RealHR_loss: 0.0395, D_FakeHR_loss: 0.2146, D_RealHRM_loss: 0.0160, D_FakeHRM_loss: 0.0142\n",
      "GAN_loss: 1.4283, G_L1_LR_loss: 0.1140, G_L1_HR_loss: 0.1458, G_L1_HRM_loss: 0.0491, G_vgg_HR_loss: 0.7172 \n",
      "Epoch [24/1001], BatchStep[400/450]\n",
      "D_loss: 0.1527, G_loss: 11.3977\n",
      "D_RealHR_loss: 0.4836, D_FakeHR_loss: 0.1257, D_RealHRM_loss: 0.0010, D_FakeHRM_loss: 0.0005\n",
      "GAN_loss: 1.3293, G_L1_LR_loss: 0.1148, G_L1_HR_loss: 0.1430, G_L1_HRM_loss: 0.0482, G_vgg_HR_loss: 0.7009 \n",
      "Epoch [25/1001], BatchStep[100/450]\n",
      "D_loss: 0.0849, G_loss: 11.8633\n",
      "D_RealHR_loss: 0.0186, D_FakeHR_loss: 0.3098, D_RealHRM_loss: 0.0060, D_FakeHRM_loss: 0.0053\n",
      "GAN_loss: 1.6594, G_L1_LR_loss: 0.1162, G_L1_HR_loss: 0.1473, G_L1_HRM_loss: 0.0480, G_vgg_HR_loss: 0.7088 \n",
      "Epoch [25/1001], BatchStep[200/450]\n",
      "D_loss: 0.0284, G_loss: 11.9356\n",
      "D_RealHR_loss: 0.0783, D_FakeHR_loss: 0.0318, D_RealHRM_loss: 0.0018, D_FakeHRM_loss: 0.0016\n",
      "GAN_loss: 1.6720, G_L1_LR_loss: 0.1177, G_L1_HR_loss: 0.1499, G_L1_HRM_loss: 0.0512, G_vgg_HR_loss: 0.7075 \n",
      "Epoch [25/1001], BatchStep[300/450]\n",
      "D_loss: 0.0364, G_loss: 11.7850\n",
      "D_RealHR_loss: 0.0581, D_FakeHR_loss: 0.0843, D_RealHRM_loss: 0.0025, D_FakeHRM_loss: 0.0009\n",
      "GAN_loss: 1.5758, G_L1_LR_loss: 0.1324, G_L1_HR_loss: 0.1474, G_L1_HRM_loss: 0.0508, G_vgg_HR_loss: 0.6902 \n",
      "Epoch [25/1001], BatchStep[400/450]\n",
      "D_loss: 0.0426, G_loss: 11.9879\n",
      "D_RealHR_loss: 0.0546, D_FakeHR_loss: 0.1114, D_RealHRM_loss: 0.0033, D_FakeHRM_loss: 0.0010\n",
      "GAN_loss: 1.8410, G_L1_LR_loss: 0.1103, G_L1_HR_loss: 0.1408, G_L1_HRM_loss: 0.0432, G_vgg_HR_loss: 0.7203 \n",
      "Epoch [26/1001], BatchStep[100/450]\n",
      "D_loss: 0.0445, G_loss: 11.7921\n",
      "D_RealHR_loss: 0.0784, D_FakeHR_loss: 0.0956, D_RealHRM_loss: 0.0025, D_FakeHRM_loss: 0.0013\n",
      "GAN_loss: 1.6301, G_L1_LR_loss: 0.1159, G_L1_HR_loss: 0.1405, G_L1_HRM_loss: 0.0467, G_vgg_HR_loss: 0.7130 \n",
      "Epoch [26/1001], BatchStep[200/450]\n",
      "D_loss: 0.0555, G_loss: 13.6338\n",
      "D_RealHR_loss: 0.1701, D_FakeHR_loss: 0.0334, D_RealHRM_loss: 0.0100, D_FakeHRM_loss: 0.0084\n",
      "GAN_loss: 1.2778, G_L1_LR_loss: 0.1170, G_L1_HR_loss: 0.1794, G_L1_HRM_loss: 0.0619, G_vgg_HR_loss: 0.8773 \n",
      "Epoch [26/1001], BatchStep[300/450]\n",
      "D_loss: 0.0205, G_loss: 11.6951\n",
      "D_RealHR_loss: 0.0528, D_FakeHR_loss: 0.0223, D_RealHRM_loss: 0.0043, D_FakeHRM_loss: 0.0027\n",
      "GAN_loss: 1.8023, G_L1_LR_loss: 0.1036, G_L1_HR_loss: 0.1350, G_L1_HRM_loss: 0.0460, G_vgg_HR_loss: 0.7046 \n",
      "Epoch [26/1001], BatchStep[400/450]\n",
      "D_loss: 0.0170, G_loss: 12.0474\n",
      "D_RealHR_loss: 0.0170, D_FakeHR_loss: 0.0476, D_RealHRM_loss: 0.0025, D_FakeHRM_loss: 0.0011\n",
      "GAN_loss: 2.1186, G_L1_LR_loss: 0.1085, G_L1_HR_loss: 0.1373, G_L1_HRM_loss: 0.0445, G_vgg_HR_loss: 0.7025 \n",
      "Epoch [27/1001], BatchStep[100/450]\n",
      "D_loss: 0.0447, G_loss: 15.0171\n",
      "D_RealHR_loss: 0.1199, D_FakeHR_loss: 0.0554, D_RealHRM_loss: 0.0021, D_FakeHRM_loss: 0.0012\n",
      "GAN_loss: 1.5199, G_L1_LR_loss: 0.1220, G_L1_HR_loss: 0.1687, G_L1_HRM_loss: 0.0560, G_vgg_HR_loss: 1.0030 \n",
      "Epoch [27/1001], BatchStep[200/450]\n",
      "D_loss: 0.0331, G_loss: 11.5771\n",
      "D_RealHR_loss: 0.0886, D_FakeHR_loss: 0.0411, D_RealHRM_loss: 0.0015, D_FakeHRM_loss: 0.0011\n",
      "GAN_loss: 1.5334, G_L1_LR_loss: 0.1151, G_L1_HR_loss: 0.1398, G_L1_HRM_loss: 0.0458, G_vgg_HR_loss: 0.7037 \n",
      "Epoch [27/1001], BatchStep[300/450]\n",
      "D_loss: 0.0325, G_loss: 11.7212\n",
      "D_RealHR_loss: 0.0556, D_FakeHR_loss: 0.0718, D_RealHRM_loss: 0.0012, D_FakeHRM_loss: 0.0013\n",
      "GAN_loss: 1.6041, G_L1_LR_loss: 0.1156, G_L1_HR_loss: 0.1406, G_L1_HRM_loss: 0.0471, G_vgg_HR_loss: 0.7084 \n",
      "Epoch [27/1001], BatchStep[400/450]\n",
      "D_loss: 0.0382, G_loss: 12.1718\n",
      "D_RealHR_loss: 0.0303, D_FakeHR_loss: 0.1204, D_RealHRM_loss: 0.0016, D_FakeHRM_loss: 0.0005\n",
      "GAN_loss: 2.1659, G_L1_LR_loss: 0.1071, G_L1_HR_loss: 0.1397, G_L1_HRM_loss: 0.0452, G_vgg_HR_loss: 0.7086 \n",
      "Epoch [28/1001], BatchStep[100/450]\n",
      "D_loss: 0.0296, G_loss: 11.4091\n",
      "D_RealHR_loss: 0.0574, D_FakeHR_loss: 0.0589, D_RealHRM_loss: 0.0014, D_FakeHRM_loss: 0.0006\n",
      "GAN_loss: 1.7431, G_L1_LR_loss: 0.1037, G_L1_HR_loss: 0.1363, G_L1_HRM_loss: 0.0437, G_vgg_HR_loss: 0.6829 \n",
      "Epoch [28/1001], BatchStep[200/450]\n",
      "D_loss: 0.0685, G_loss: 11.7146\n",
      "D_RealHR_loss: 0.1659, D_FakeHR_loss: 0.1060, D_RealHRM_loss: 0.0016, D_FakeHRM_loss: 0.0004\n",
      "GAN_loss: 1.8641, G_L1_LR_loss: 0.1042, G_L1_HR_loss: 0.1410, G_L1_HRM_loss: 0.0453, G_vgg_HR_loss: 0.6945 \n",
      "Epoch [28/1001], BatchStep[300/450]\n",
      "D_loss: 0.0325, G_loss: 12.2426\n",
      "D_RealHR_loss: 0.0123, D_FakeHR_loss: 0.1164, D_RealHRM_loss: 0.0008, D_FakeHRM_loss: 0.0005\n",
      "GAN_loss: 2.2700, G_L1_LR_loss: 0.1090, G_L1_HR_loss: 0.1426, G_L1_HRM_loss: 0.0454, G_vgg_HR_loss: 0.7002 \n",
      "Epoch [28/1001], BatchStep[400/450]\n",
      "D_loss: 0.0253, G_loss: 13.1856\n",
      "D_RealHR_loss: 0.0411, D_FakeHR_loss: 0.0536, D_RealHRM_loss: 0.0039, D_FakeHRM_loss: 0.0024\n",
      "GAN_loss: 2.1825, G_L1_LR_loss: 0.1113, G_L1_HR_loss: 0.1657, G_L1_HRM_loss: 0.0539, G_vgg_HR_loss: 0.7695 \n",
      "Epoch [29/1001], BatchStep[100/450]\n",
      "D_loss: 0.0193, G_loss: 11.7410\n",
      "D_RealHR_loss: 0.0223, D_FakeHR_loss: 0.0502, D_RealHRM_loss: 0.0040, D_FakeHRM_loss: 0.0009\n",
      "GAN_loss: 1.9317, G_L1_LR_loss: 0.1080, G_L1_HR_loss: 0.1375, G_L1_HRM_loss: 0.0454, G_vgg_HR_loss: 0.6900 \n",
      "Epoch [29/1001], BatchStep[200/450]\n",
      "D_loss: 0.0370, G_loss: 11.5525\n",
      "D_RealHR_loss: 0.0870, D_FakeHR_loss: 0.0600, D_RealHRM_loss: 0.0004, D_FakeHRM_loss: 0.0004\n",
      "GAN_loss: 1.6526, G_L1_LR_loss: 0.1146, G_L1_HR_loss: 0.1397, G_L1_HRM_loss: 0.0489, G_vgg_HR_loss: 0.6868 \n",
      "Epoch [29/1001], BatchStep[300/450]\n",
      "D_loss: 0.0454, G_loss: 11.8371\n",
      "D_RealHR_loss: 0.0106, D_FakeHR_loss: 0.1662, D_RealHRM_loss: 0.0031, D_FakeHRM_loss: 0.0018\n",
      "GAN_loss: 2.0336, G_L1_LR_loss: 0.1034, G_L1_HR_loss: 0.1369, G_L1_HRM_loss: 0.0425, G_vgg_HR_loss: 0.6976 \n",
      "Epoch [29/1001], BatchStep[400/450]\n",
      "D_loss: 0.0109, G_loss: 11.6507\n",
      "D_RealHR_loss: 0.0150, D_FakeHR_loss: 0.0272, D_RealHRM_loss: 0.0009, D_FakeHRM_loss: 0.0007\n",
      "GAN_loss: 1.9157, G_L1_LR_loss: 0.1037, G_L1_HR_loss: 0.1388, G_L1_HRM_loss: 0.0484, G_vgg_HR_loss: 0.6826 \n",
      "Epoch [30/1001], BatchStep[100/450]\n",
      "D_loss: 0.0137, G_loss: 11.6685\n",
      "D_RealHR_loss: 0.0265, D_FakeHR_loss: 0.0252, D_RealHRM_loss: 0.0021, D_FakeHRM_loss: 0.0010\n",
      "GAN_loss: 1.9789, G_L1_LR_loss: 0.1030, G_L1_HR_loss: 0.1378, G_L1_HRM_loss: 0.0458, G_vgg_HR_loss: 0.6824 \n",
      "Epoch [30/1001], BatchStep[200/450]\n",
      "D_loss: 0.0398, G_loss: 11.2216\n",
      "D_RealHR_loss: 0.0454, D_FakeHR_loss: 0.1125, D_RealHRM_loss: 0.0005, D_FakeHRM_loss: 0.0009\n",
      "GAN_loss: 1.6445, G_L1_LR_loss: 0.1016, G_L1_HR_loss: 0.1318, G_L1_HRM_loss: 0.0425, G_vgg_HR_loss: 0.6818 \n",
      "Epoch [30/1001], BatchStep[300/450]\n",
      "D_loss: 0.0129, G_loss: 11.8179\n",
      "D_RealHR_loss: 0.0195, D_FakeHR_loss: 0.0311, D_RealHRM_loss: 0.0004, D_FakeHRM_loss: 0.0005\n",
      "GAN_loss: 1.8648, G_L1_LR_loss: 0.1047, G_L1_HR_loss: 0.1381, G_L1_HRM_loss: 0.0432, G_vgg_HR_loss: 0.7094 \n",
      "Epoch [30/1001], BatchStep[400/450]\n",
      "D_loss: 0.0241, G_loss: 11.4180\n",
      "D_RealHR_loss: 0.0505, D_FakeHR_loss: 0.0446, D_RealHRM_loss: 0.0009, D_FakeHRM_loss: 0.0003\n",
      "GAN_loss: 1.5655, G_L1_LR_loss: 0.1026, G_L1_HR_loss: 0.1401, G_L1_HRM_loss: 0.0449, G_vgg_HR_loss: 0.6976 \n",
      "Epoch [31/1001], BatchStep[100/450]\n",
      "D_loss: 0.0296, G_loss: 11.5776\n",
      "D_RealHR_loss: 0.0217, D_FakeHR_loss: 0.0941, D_RealHRM_loss: 0.0015, D_FakeHRM_loss: 0.0010\n",
      "GAN_loss: 1.7972, G_L1_LR_loss: 0.1112, G_L1_HR_loss: 0.1403, G_L1_HRM_loss: 0.0460, G_vgg_HR_loss: 0.6805 \n",
      "Epoch [31/1001], BatchStep[200/450]\n",
      "D_loss: 0.0395, G_loss: 11.9262\n",
      "D_RealHR_loss: 0.0937, D_FakeHR_loss: 0.0227, D_RealHRM_loss: 0.0208, D_FakeHRM_loss: 0.0210\n",
      "GAN_loss: 1.8197, G_L1_LR_loss: 0.0976, G_L1_HR_loss: 0.1341, G_L1_HRM_loss: 0.0448, G_vgg_HR_loss: 0.7342 \n",
      "Epoch [31/1001], BatchStep[300/450]\n",
      "D_loss: 0.0311, G_loss: 11.7042\n",
      "D_RealHR_loss: 0.0268, D_FakeHR_loss: 0.0970, D_RealHRM_loss: 0.0005, D_FakeHRM_loss: 0.0003\n",
      "GAN_loss: 1.7567, G_L1_LR_loss: 0.1121, G_L1_HR_loss: 0.1398, G_L1_HRM_loss: 0.0466, G_vgg_HR_loss: 0.6963 \n",
      "Epoch [31/1001], BatchStep[400/450]\n",
      "D_loss: 0.0125, G_loss: 11.3452\n",
      "D_RealHR_loss: 0.0226, D_FakeHR_loss: 0.0246, D_RealHRM_loss: 0.0017, D_FakeHRM_loss: 0.0009\n",
      "GAN_loss: 1.6940, G_L1_LR_loss: 0.1098, G_L1_HR_loss: 0.1348, G_L1_HRM_loss: 0.0426, G_vgg_HR_loss: 0.6780 \n"
     ]
    }
   ],
   "source": [
    "# tensorboardX summary\n",
    "summary = SummaryWriter('SRnDeblur_Neverywhere/log')\n",
    "# Networks\n",
    "# generator = nn.DataParallel(Generator(args.batchSize))\n",
    "# discriminator = nn.DataParallel(Discriminator(6,args.batchSize))\n",
    "# discriminatorM = nn.DataParallel(Discriminator(6,args.batchSize))\n",
    "generator = nn.DataParallel(Generator(args.batchSize))\n",
    "discriminatorHR = nn.DataParallel(Discriminator(6,args.batchSize))\n",
    "discriminatorHRM = nn.DataParallel(Discriminator(3,args.batchSize))\n",
    "\n",
    "# dummmy = torch.zeros(1,3,32,32)\n",
    "# summary.add_graph(generator,dummmy)\n",
    "# summary.add_graph(discriminator,(dummmy,dummmy))\n",
    "\n",
    "# Losses -> vanilaGAN의 loss, MSE는 LSGAN의 loss\n",
    "# criterionGAN = nn.BCELoss() \n",
    "criterionGAN = nn.MSELoss()\n",
    "criterionL1 = nn.L1Loss()\n",
    "vgg_loss = VGGLoss()\n",
    "# Optimizers\n",
    "d_parameters = list(discriminatorHR.parameters()) + list(discriminatorHRM.parameters())\n",
    "g_optimizer = optim.Adam(generator.parameters(), args.lr, [args.beta1, args.beta2])\n",
    "d_optimizer = optim.Adam(d_parameters, args.lr, [args. beta1, args.beta2])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = generator.cuda()\n",
    "    discriminatorHR = discriminatorHR.cuda()\n",
    "    discriminatorHRM = discriminatorHRM.cuda()\n",
    "    \n",
    "### train generator and discriminator\n",
    "# for printing the log\n",
    "total_step = len(data_loader)\n",
    "\n",
    "for epoch  in range(args.num_epochs):\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        # A : 32x32 (blur + LR)\n",
    "        # B : 256x256 (LR)\n",
    "        # C : 256x256 (GT)\n",
    "        # D : 256x256 (fmask)\n",
    "        input_A = sample['A']\n",
    "        GTLR = sample['B']\n",
    "        GTHR = sample['C']\n",
    "        fmaskGT = sample['D'].to(\"cuda\")\n",
    "        inputbi = sample['E']\n",
    "        \n",
    "        #======================== preparing ========================#\n",
    "        in_blurLR = to_variable(input_A)\n",
    "        upx, fakeLR, fakeHR = generator(in_blurLR)\n",
    "        v_GTLR = to_variable(GTLR)\n",
    "        v_GTHR = to_variable(GTHR)\n",
    "        v_inputbi = to_variable(inputbi)\n",
    "        fakeHR_md = fakeHR * fmaskGT\n",
    "        GTHR_md = v_GTHR * fmaskGT\n",
    "        \n",
    "        #========================= train D =========================#\n",
    "        # zero_grad : 역전파 실행 전 변화도 0으로 만듦\n",
    "        discriminatorHR.zero_grad()\n",
    "        discriminatorHRM.zero_grad()\n",
    "        \n",
    "        pred_fakeHR = discriminatorHR(torch.cat((upx.detach(), fakeHR.detach()),dim=1))\n",
    "        loss_D_fake = GAN_Loss(pred_fakeHR, False, criterionGAN)\n",
    "        pred_realHR = discriminatorHR(torch.cat((upx.detach(), v_GTHR),dim=1))\n",
    "        loss_D_real = GAN_Loss(pred_realHR, True, criterionGAN)\n",
    "        \n",
    "        pred_fakeM = discriminatorHRM(fakeHR_md.detach())\n",
    "        loss_D_fakeM = GAN_Loss(pred_fakeM, False, criterionGAN)\n",
    "        pred_realM = discriminatorHRM(GTHR_md)\n",
    "        loss_D_realM = GAN_Loss(pred_realM, True, criterionGAN)\n",
    "        \n",
    "        # combined loss\n",
    "        loss_D = (loss_D_fake + loss_D_real + loss_D_fakeM + loss_D_realM) * 0.25 #0.25 & 0.5\n",
    "        loss_D.backward()\n",
    "        d_optimizer.step()\n",
    "        #========================= train G =========================#\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        pred_fake = discriminatorHR(torch.cat((upx, fakeHR), dim=1))\n",
    "        pred_fakeM = discriminatorHRM(fakeHR_md)\n",
    "        loss_G_GAN = GAN_Loss(pred_fake, True, criterionGAN) + GAN_Loss(pred_fakeM, True, criterionGAN)\n",
    "        \n",
    "        loss_G_L1_LR = criterionL1(fakeLR, v_GTLR)\n",
    "        loss_G_L1_HR = criterionL1(fakeHR, v_GTHR)\n",
    "        loss_G_L1_HRM = criterionL1(fakeHR_md, GTHR_md)\n",
    "        \n",
    "        loss_G_vgg_HR = vgg_loss(fakeHR, v_GTHR)\n",
    "        \n",
    "        loss_G = loss_G_GAN + (loss_G_L1_LR + loss_G_L1_HR + loss_G_L1_HRM) * args.lambda_A + loss_G_vgg_HR * 10.0\n",
    "        \n",
    "        loss_G.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # print the log information\n",
    "        if (i+1) % args.log_step == 0:\n",
    "            print('Epoch [%d/%d], BatchStep[%d/%d]' % (epoch + 1, args.num_epochs, i + 1, total_step))\n",
    "            print('D_loss: %.4f, G_loss: %.4f' % (loss_D.data, loss_G.data))\n",
    "            print('D_RealHR_loss: %.4f, D_FakeHR_loss: %.4f, D_RealHRM_loss: %.4f, D_FakeHRM_loss: %.4f' \n",
    "                  % (loss_D_real.data, loss_D_fake.data, loss_D_realM.data, loss_D_fakeM.data))          \n",
    "            print('GAN_loss: %.4f, G_L1_LR_loss: %.4f, G_L1_HR_loss: %.4f, G_L1_HRM_loss: %.4f, G_vgg_HR_loss: %.4f '\n",
    "                  % (loss_G_GAN.data, loss_G_L1_LR.data, loss_G_L1_HR.data,loss_G_L1_HRM.data, loss_G_vgg_HR.data))\n",
    "            \n",
    "        # save the sampled images\n",
    "        if (i+1)%args.sample_step == 0:\n",
    "            in_Ar = upx[0:4,:,:,:]\n",
    "            fake_Br = fakeHR[0:4,:,:,:]\n",
    "            real_Br = v_GTHR[0:4,:,:,:]\n",
    "            bilinear_in = v_inputbi[0:4,:,:,:]\n",
    "            \n",
    "            resHR = torch.cat((torch.cat((in_Ar, fake_Br),dim=3), real_Br), dim=3)\n",
    "            resLR = torch.cat((torch.cat((bilinear_in, fake_Br),dim=3), real_Br),dim=3)\n",
    "            torchvision.utils.save_image(denorm(resHR.data), os.path.join(args.sample_path, 'HRwF-%d-%d.png' % (epoch + 1, i + 1)))\n",
    "            torchvision.utils.save_image(denorm(resLR.data), os.path.join(args.sample_path, 'HRwB-%d-%d.png' % (epoch + 1, i + 1)))\n",
    "\n",
    "#             resMasked_0 = torch.cat((torch.cat((torch.cat((fake_Br,real_Br),dim=2),fmasked_FB[0:4,:,:,:]),dim=2),fmasked_GT[0:4,:,:,:]),dim=2)\n",
    "#             resMasked = torch.cat((torch.cat((resMasked_0,parmasked_FB[0:4,:,:,:]),dim=2),parmasked_FB[0:4,:,:,:]),dim=2)\n",
    "#             torchvision.utils.save_image(denorm(resMasked.data), os.path.join(args.sample_path, 'Generated_RM-%d-%d.png' % (epoch + 1, i + 1)))\n",
    "            \n",
    "    # save summary\n",
    "    summary.add_scalar('loss/loss_D_real', loss_D_real.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_D_fake', loss_D_fake.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_D_realM', loss_D_realM.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_D_fakeM', loss_D_fakeM.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_D', loss_D.item(), epoch)\n",
    "    \n",
    "    summary.add_scalar('loss/loss_G_GAN', loss_G_GAN.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_G_L1_LR', loss_G_L1_LR.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_G_L1_HR', loss_G_L1_HR.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_G_L1_HRM', loss_G_L1_HRM.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_G_vgg_HR', loss_G_vgg_HR.item(), epoch)\n",
    "    summary.add_scalar('loss/loss_G', loss_G.item(), epoch)\n",
    "    \n",
    "    fakeOutHR = make_grid(fake_Br, normalize=True, scale_each=True)\n",
    "    realGTHR = make_grid(real_Br, normalize=True, scale_each=True)\n",
    "\n",
    "    \n",
    "    summary.add_image('0_GT_HR', realGTHR, epoch)\n",
    "    summary.add_image('1_generated HR', fakeOutHR, epoch)\n",
    "\n",
    "    \n",
    "    # save the model parameters\n",
    "    if epoch % 10 == 0:\n",
    "        g_path = os.path.join(args.model_path, 'generator-%d.pkl' % (epoch + 1))\n",
    "        torch.save(generator.state_dict(), g_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
