{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import inception_v3\n",
    "import cv2\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(elements):\n",
    "    \"\"\"\n",
    "    Transfers elements to cuda if GPU is available\n",
    "    Args:\n",
    "        elements: torch.tensor or torch.nn.module\n",
    "        --\n",
    "    Returns:\n",
    "        elements: same as input on GPU memory, if available\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return elements.cuda()\n",
    "    return elements\n",
    "\n",
    "class PartialInceptionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, transform_input=True):\n",
    "        super().__init__()\n",
    "        self.inception_network = inception_v3(pretrained=True)\n",
    "        self.inception_network.Mixed_7c.register_forward_hook(self.output_hook)\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "    def output_hook(self, module, input, output):\n",
    "        # N x 2048 x 8 x 8\n",
    "        self.mixed_7c_output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (N, 3, 299, 299) dtype: torch.float32 in range 0-1\n",
    "        Returns:\n",
    "            inception activations: torch.tensor, shape: (N, 2048), dtype: torch.float32\n",
    "        \"\"\"\n",
    "        assert x.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\n",
    "                                             \", but got {}\".format(x.shape)\n",
    "        x = x * 2 -1 # Normalize to [-1, 1]\n",
    "\n",
    "        # Trigger output hook\n",
    "        self.inception_network(x)\n",
    "\n",
    "        # Output: N x 2048 x 1 x 1 \n",
    "        activations = self.mixed_7c_output\n",
    "        activations = torch.nn.functional.adaptive_avg_pool2d(activations, (1,1))\n",
    "        activations = activations.view(x.shape[0], 2048)\n",
    "        return activations\n",
    "\n",
    "\n",
    "def get_activations(images, batch_size):\n",
    "    \"\"\"\n",
    "    Calculates activations for last pool layer for all iamges\n",
    "    --\n",
    "        Images: torch.array shape: (N, 3, 299, 299), dtype: torch.float32\n",
    "        batch size: batch size used for inception network\n",
    "    --\n",
    "    Returns: np array shape: (N, 2048), dtype: np.float32\n",
    "    \"\"\"\n",
    "    assert images.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\n",
    "                                              \", but got {}\".format(images.shape)\n",
    "\n",
    "    num_images = images.shape[0]\n",
    "    inception_network = PartialInceptionNetwork()\n",
    "    inception_network = to_cuda(inception_network)\n",
    "    inception_network.eval()\n",
    "    n_batches = int(np.ceil(num_images  / batch_size))\n",
    "    inception_activations = np.zeros((num_images, 2048), dtype=np.float32)\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_size * batch_idx\n",
    "        end_idx = batch_size * (batch_idx + 1)\n",
    "\n",
    "        ims = images[start_idx:end_idx]\n",
    "        ims = to_cuda(ims)\n",
    "        activations = inception_network(ims)\n",
    "        activations = activations.detach().cpu().numpy()\n",
    "        assert activations.shape == (ims.shape[0], 2048), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 2048), activations.shape)\n",
    "        inception_activations[start_idx:end_idx, :] = activations\n",
    "    return inception_activations\n",
    "\n",
    "def calculate_activation_statistics(images, batch_size):\n",
    "    \"\"\"Calculates the statistics used by FID\n",
    "    Args:\n",
    "        images: torch.tensor, shape: (N, 3, H, W), dtype: torch.float32 in range 0 - 1\n",
    "        batch_size: batch size to use to calculate inception scores\n",
    "    Returns:\n",
    "        mu:     mean over all activations from the last pool layer of the inception model\n",
    "        sigma:  covariance matrix over all activations from the last pool layer \n",
    "                of the inception model.\n",
    "    \"\"\"\n",
    "    act = get_activations(images, batch_size)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "            \n",
    "    Stable version by Dougal J. Sutherland.\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "def preprocess_image(im):\n",
    "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
    "    Args:\n",
    "        im: np.array, shape: (H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
    "    Return:\n",
    "        im: torch.tensor, shape: (3, 299, 299), dtype: torch.float32 between 0-1\n",
    "    \"\"\"\n",
    "    assert im.shape[2] == 3\n",
    "    assert len(im.shape) == 3\n",
    "    if im.dtype == np.uint8:\n",
    "        im = im.astype(np.float32) / 255\n",
    "    im = cv2.resize(im, (299, 299))\n",
    "    im = np.rollaxis(im, axis=2)\n",
    "    im = torch.from_numpy(im)\n",
    "    assert im.max() <= 1.0\n",
    "    assert im.min() >= 0.0\n",
    "    assert im.dtype == torch.float32\n",
    "    assert im.shape == (3, 299, 299)\n",
    "\n",
    "    return im\n",
    "\n",
    "def preprocess_images(images, use_multiprocessing):\n",
    "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
    "    Args:\n",
    "        images: np.array, shape: (N, H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
    "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
    "    Return:\n",
    "        final_images: torch.tensor, shape: (N, 3, 299, 299), dtype: torch.float32 between 0-1\n",
    "    \"\"\"\n",
    "    if use_multiprocessing:\n",
    "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "            jobs = []\n",
    "            for im in images:\n",
    "                job = pool.apply_async(preprocess_image, (im,))\n",
    "                jobs.append(job)\n",
    "            final_images = torch.zeros(images.shape[0], 3, 299, 299)\n",
    "            for idx, job in enumerate(jobs):\n",
    "                im = job.get()\n",
    "                final_images[idx] = im#job.get()\n",
    "    else:\n",
    "        final_images = torch.stack([preprocess_image(im) for im in images], dim=0)\n",
    "    assert final_images.shape == (images.shape[0], 3, 299, 299)\n",
    "    assert final_images.max() <= 1.0\n",
    "    assert final_images.min() >= 0.0\n",
    "    assert final_images.dtype == torch.float32\n",
    "    return final_images\n",
    "\n",
    "def calculate_fid(images1, images2, use_multiprocessing, batch_size):\n",
    "    \"\"\" Calculate FID between images1 and images2\n",
    "    Args:\n",
    "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
    "        batch size: batch size used for inception network\n",
    "    Returns:\n",
    "        FID (scalar)\n",
    "    \"\"\"\n",
    "    images1 = preprocess_images(images1, use_multiprocessing)\n",
    "    images2 = preprocess_images(images2, use_multiprocessing)\n",
    "    mu1, sigma1 = calculate_activation_statistics(images1, batch_size)\n",
    "    mu2, sigma2 = calculate_activation_statistics(images2, batch_size)\n",
    "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return fid\n",
    "\n",
    "def load_images(path):\n",
    "    \"\"\" Loads all .png or .jpg images from a given path\n",
    "    Warnings: Expects all images to be of same dtype and shape.\n",
    "    Args:\n",
    "        path: relative path to directory\n",
    "    Returns:\n",
    "        final_images: np.array of image dtype and shape.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_extensions = [\"png\", \"jpg\"]\n",
    "    for ext in image_extensions:\n",
    "        print(\"Looking for images in\", os.path.join(path, \"*.{}\".format(ext)))\n",
    "        for impath in glob.glob(os.path.join(path, \"*.{}\".format(ext))):\n",
    "            image_paths.append(impath)\n",
    "    first_image = cv2.imread(image_paths[0])\n",
    "    \n",
    "    W, H = first_image.shape[:2]\n",
    "    image_paths.sort()\n",
    "    image_paths = image_paths\n",
    "    final_images = np.zeros((len(image_paths), H, W, 3), dtype=first_image.dtype)\n",
    "    final_images1 = np.zeros((len(image_paths), H, W, 3), dtype=first_image.dtype)\n",
    "    \n",
    "    for idx, impath in enumerate(image_paths):\n",
    "        trn = impath.find('val_GT')\n",
    "        endn = len(impath)\n",
    "        impath1 = './comparison_results/SRGAN_re/'+impath[trn+7:endn-4]+'_pred.png'\n",
    "        im = cv2.imread(impath)\n",
    "        im1 = cv2.imread(impath1)\n",
    "        im = im[:, :, ::-1] # Convert from BGR to RGB\n",
    "        im1 = im1[:, :, ::-1] \n",
    "        assert im.dtype == final_images.dtype\n",
    "        final_images[idx] = im\n",
    "        assert im1.dtype == final_images1.dtype\n",
    "        final_images1[idx] = im1\n",
    "        \n",
    "        \n",
    "    return final_images, final_images1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='FID')\n",
    "\n",
    "parser.add_argument('--path1', type=str, default='./datasets/face_SRnDeblur/val_GT')\n",
    "parser.add_argument('--path2', type=str, default='./comparison_results/wnoise/')\n",
    "\n",
    "parser.add_argument('--multiprocessing', action='store_true', default=False)\n",
    "parser.add_argument('--batch_size',type=int,default=8)\n",
    "\n",
    "global args\n",
    "args = parser.parse_args(['--path1','./datasets/face_SRnDeblur/val_GT',\n",
    "                          '--path2', './comparison_results/SRResNet_re/',\n",
    "                          '--batch_size','8'])\n",
    "\n",
    "images1, images2 = load_images(args.path1)\n",
    "\n",
    "fid_value = calculate_fid(images1, images2, args.multiprocessing, args.batch_size)\n",
    "print(fid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions for Data Loading & Pre-processingclass ImageFolder(data.Dataset):\n",
    "class ImageFolder(data.Dataset):\n",
    "    def __init__(self, opt):\n",
    "        # os.listdir function gives all lists of directory\n",
    "        self.root = opt.dataroot\n",
    "        self.no_resize_or_crop = opt.no_resize_or_crop\n",
    "        self.no_flip = opt.no_flip\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "        self.transformM = transforms.Compose([transforms.ToTensor()])\n",
    "        #=====================================================================================#\n",
    "        self.dir_A = os.path.join(opt.dataroot,'valx8')\n",
    "        self.Aimg_paths = list(map(lambda x:os.path.join(self.dir_A,x),os.listdir(self.dir_A)))\n",
    "        #=====================================================================================#\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #=====================================================================================#\n",
    "        # A : 32x32 (blur + LR)\n",
    "        # B : 256x256 (LR)\n",
    "        # C : 256x256 (GT)\n",
    "        # D : 256x256 (fmask)\n",
    "        A_path = self.Aimg_paths[index]\n",
    "        trn = A_path.find('valx8')\n",
    "        endn = len(A_path)\n",
    "        C_path = A_path[:trn]+'val_GT'+A_path[trn+5:endn-4]+'.jpg'\n",
    "        B_path = './comparison_results/detail1/'+A_path[trn+6:endn-4]+'_pred.png'\n",
    "        A = Image.open(A_path).convert('RGB')\n",
    "        C = Image.open(C_path).convert('RGB')\n",
    "        B = Image.open(B_path)\n",
    "        A = self.transform(A)\n",
    "        B = self.transform(B)\n",
    "        C = self.transform(C)\n",
    "\n",
    "        return {'A':A,'B':B, 'C':C,'fname':A_path[trn+6:endn-4]}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Aimg_paths)\n",
    "\n",
    "##### Helper Function for GPU Training\n",
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "##### Helper Function for Math\n",
    "def denorm(x):\n",
    "    out = (x+1)/2\n",
    "    return out.clamp(0,1)\n",
    "\n",
    "##### Helper Functions for GAN Loss (4D Loss Comparison)\n",
    "def GAN_Loss(input, target, criterion):\n",
    "    if target == True:\n",
    "        tmp_tensor = torch.FloatTensor(input.size()).fill_(1.0)\n",
    "        labels = Variable(tmp_tensor, requires_grad=False)\n",
    "    else:\n",
    "        tmp_tensor = torch.FloatTensor(input.size()).fill_(0.0)\n",
    "        labels = Variable(tmp_tensor, requires_grad=False)\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    return criterion(input, labels)\n",
    "##### Helper Function for Math\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def to_numpy(x):\n",
    "    x = x.cpu()\n",
    "    x = ((x.detach().numpy()+1)/2)\n",
    "    x = np.transpose(x,(1,2,0))\n",
    "    return x\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.linalg.norm(x - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Implementation of Pix2Pix')\n",
    "\n",
    "# Task\n",
    "parser.add_argument('--dataroot', required=True, help='path to images (should have subfolders train, val, etc)')\n",
    "parser.add_argument('--which_direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
    "\n",
    "# Options\n",
    "parser.add_argument('--no_resize_or_crop', action='store_true', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]')\n",
    "parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
    "parser.add_argument('--num_epochs', type=int, default=100)\n",
    "parser.add_argument('--batchSize', type=int, default=1, help='test Batch size')\n",
    "\n",
    "# misc\n",
    "parser.add_argument('--model_path', type=str, default='./models')\n",
    "parser.add_argument('--sample_path', type=str, default='./test_results')\n",
    "parser.add_argument('--results_txt', type=str, default='./test_MSE_PSNR_SSIM.txt')\n",
    "\n",
    "##### Helper Functions for Data Loading & Pre-processing\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "args = parser.parse_args(['--dataroot','./datasets/face_SRnDeblur','--which_direction','AtoB',\n",
    "                          '--num_epochs','451','--batchSize','16','--no_resize_or_crop',\n",
    "                          '--model_path','./Model3_1015_b16e451/models',\n",
    "                          '--sample_path','./Model3_1015_b16e451/results_e451',\n",
    "                         '--results_txt','./Model3_1015_b16e451/PSNRSSIM_e451.txt'])\n",
    "# 741 751 761 771 781\n",
    "print(args)\n",
    "\n",
    "dataset = ImageFolder(args)\n",
    "data_loader = data.DataLoader(dataset=dataset,\n",
    "                              batch_size=args.batchSize,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage import data, img_as_float\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_psnr as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_step = len(data_loader) # For Print Log\n",
    "mse_in_all, mse_out_all, psnr_in_all, psnr_out_all, ssim_in_all, ssim_out_all = 0,0,0,0,0,0\n",
    "for i, sample in enumerate(data_loader):\n",
    "\n",
    "    input_A = sample['A']\n",
    "    comp = sample['B']\n",
    "    GTHR = sample['C']\n",
    "    testfileN = sample['fname']\n",
    "\n",
    "    in_blurLR = to_variable(input_A)\n",
    "    v_comp = to_variable(comp)\n",
    "    v_GTHR = to_variable(GTHR)\n",
    "    \n",
    "    # print the log info\n",
    "    print('Validation[%d/%d]' % (i + 1, total_step))\n",
    "    # save the sampled images\n",
    "    \n",
    "    real_Br = v_GTHR[:,0:3,:,:]\n",
    "    comp_Br = v_comp[:,0:3,:,:]\n",
    "    \n",
    "    for k in range(16):\n",
    "        \n",
    "        fake_Br_ = img_as_float(to_numpy(comp_Br[k,:,:,:]))\n",
    "        real_Br_ = img_as_float(to_numpy(real_Br[k,:,:,:]))\n",
    "\n",
    "        mse_out = mse(real_Br_,fake_Br_)\n",
    "        psnr_out = psnr(real_Br_,fake_Br_,data_range=fake_Br_.max()-fake_Br_.min())\n",
    "        ssim_out = ssim(real_Br_,fake_Br_,data_range=fake_Br_.max()-fake_Br_.min(),multichannel=True)\n",
    "\n",
    "        mse_out_all += mse_out\n",
    "        psnr_out_all += psnr_out\n",
    "        ssim_out_all += ssim_out\n",
    "        \n",
    "#         torchvision.utils.save_image(denorm(comp_Br[k,:,:,:].data), os.path.join('./comparison_results/SRResNet_re', '%s.png' % testfileN[k]))\n",
    "\n",
    "print('Average of MSE PSNR SSIM \\n')\n",
    "print('mse_out : %f, psnr_out : %f, ssim_out : %f \\n' % (mse_out_all/1200, psnr_out_all/1200, ssim_out_all/1200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testfileN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
